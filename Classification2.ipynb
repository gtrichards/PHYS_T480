{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminative Classification\n",
    "\n",
    "G. Richards (2016), based on materials from Connolly, VanderPlas, and Ivezic.\n",
    "\n",
    "Last time we talked about how to do classification by mapping the full pdf of your parameter space.  This time we will concentrate on methods that seek only to determine the **decision boundary**, so called [**discriminative classification**](https://en.wikipedia.org/wiki/Discriminative_model) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As before, let's say that you have 2 blobs of data as shown below.  In many cases, you might say \"just draw a line between those two blobs that are well separated\".  So let's do exactly that in the example below.  There are clearly lots of different lines that you could draw that would work.  So, how do you do this *optimally*?  And what if the blobs are not perfectly well separated?\n",
    "\n",
    "<!-- ![Ivezic, Figure 9.9](figures/svm_lines.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://github.com/jakevdp/ESAC-stats-2014/blob/master/notebooks/04.1-Classification-SVMs.ipynb\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring');\n",
    "\n",
    "xfit = np.linspace(-1, 3.5)\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "    \n",
    "plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "This is where [Support Vector Machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) come in.  We are going to define a hyperplane (a plane in $N-1$ dimensions) that maximizes the distance of the closest point from each class.  This distance is the \"margin\".  It is the width of the \"cylinder\" that you can put between the closest points that just barely touches the points in each class.  The points that touch the margin are called **support vectors**.  Obvious, right?\n",
    "\n",
    "Once again, we have an algortihm that seems purposely named to frighten people away.  Though I don't know that \"Data-Supported Hyperplane\" classification would be any better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "N.B.: For this next part, $\\beta$ is not defined in $\\S$ 9.5.1 or $\\S$ 9.6.  In Equation 9.31 it appears to be the probability of success as defined by Equation 3.50 in $\\S$ 3.3.3.  But later $\\beta_0$ appears to be a multi-dimensional intercept and $\\beta$ appears to be a multi-dimensional slope parameter, i.e., the parameters of the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To make life easier, we'll assume that the classes are separable by a straight line and that the decision boundary is at 0, with the two edges at $-1$ and $1$, and define $y \\in \\{-1,1\\}$.\n",
    "\n",
    "The maximum is then just when $\\beta_0 + \\beta^T x_i = 1$ etc\n",
    "\n",
    "The hyperplane which maximizes the margin is given by finding\n",
    "\n",
    "> \\begin{equation}\n",
    "\\max_{\\beta_0,\\beta}(m) \\;\\;\\;\n",
    "  \\mbox{subject to} \\;\\;\\; \\frac{1}{||\\beta||} y_i ( \\beta_0 + \\beta^T x_i )\n",
    "  \\geq m \\,\\,\\, \\forall \\, i.\n",
    "\\end{equation}\n",
    "\n",
    "The constraints can be written as $y_i ( \\beta_0 + \\beta^T x_i ) \\geq m ||\\beta|| $. \n",
    "\n",
    "Thus the optimization problem is equivalent to minimizing\n",
    "> \\begin{equation}\n",
    "\\frac{1}{2} ||\\beta|| \\;\\;\\; \\mbox{subject to} \\;\\;\\; y_i\n",
    "  ( \\beta_0 + \\beta^T x_i ) \\geq 1 \\,\\,\\, \\forall \\, i.\n",
    "\\end{equation}\n",
    "\n",
    "This optimization  is a _quadratic programming_ problem (quadratic objective function with linear constraints)\n",
    "\n",
    "For realistic data sets where the decision boundary is not obvious we relax the assumption that the classes are linearly separable.  This changes the minimization condition and puts bounds on the number of misclassifications (which we could obviously like to minimize)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make sure that we get through all the remaining classification algorithms, we'll skip over the mathematical details.  You can read about them in Ivezic $\\S$ 9.6 or in Karen Leighly's [classification lecture notes](http://seminar.ouml.org/lectures/classification/).\n",
    "\n",
    "Treating Scikit-Learn's agorithm as a black box, let's fit a Support Vector Machine Classifier to these points.\n",
    "\n",
    "The Scikit-Learn implementation of SVM classification is [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) which looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to better visualize what SVM is doing, let's create a convenience function that will plot the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    u = np.linspace(plt.xlim()[0], plt.xlim()[1], 3)\n",
    "    v = np.linspace(plt.ylim()[0], plt.ylim()[1], 3)\n",
    "    yy, xx = np.meshgrid(v, u)\n",
    "\n",
    "    P = np.zeros_like(xx)\n",
    "    for i, ui in enumerate(u):\n",
    "        for j, vj in enumerate(v):\n",
    "            Xgrid = np.array([ui, vj])\n",
    "            P[i, j] = clf.decision_function(Xgrid.reshape(1,-1))\n",
    "    return ax.contour(xx, yy, P, colors='k',\n",
    "                      levels=[-1, 0, 1], alpha=0.5,\n",
    "                      linestyles=['--', '-', '--'])\n",
    "\n",
    "    #GTR: Not clear why we need the loops and can't just\n",
    "    # make the Xgrid array like we normally do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's plot the decision boundary and the support vectors, which are stored in the `support_vectors_` attribute of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plot_svc_decision_function(svm)\n",
    "plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=200, facecolors='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below is an example using the same data set from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Ivezic, Figure 9.10\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "\n",
    "# SVM takes several minutes to run, and is order[N^2]\n",
    "#  truncating the dataset can be useful for experimentation.\n",
    "X = X[::5]\n",
    "y = y[::5]\n",
    "\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "\n",
    "N_tot = len(y)\n",
    "N_stars = np.sum(y == 0)\n",
    "N_rrlyrae = N_tot - N_stars\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rrlyrae\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Fit SVM\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "\n",
    "def compute_SVM(Ncolors):\n",
    "    y_class = []\n",
    "    y_pred = []\n",
    "\n",
    "    for nc in Ncolors:\n",
    "        # perform support vector classification\n",
    "        svm = SVC(kernel='linear', class_weight='balanced')\n",
    "        svm.fit(X_train[:, :nc], y_train)\n",
    "        \n",
    "        y_pred.append(svm.predict(X_test[:, :nc]))\n",
    "        y_class.append(svm)\n",
    "\n",
    "    return y_class, y_pred\n",
    "\n",
    "y_class, y_pred = compute_SVM(Ncolors)\n",
    "\n",
    "completeness, contamination = completeness_contamination(y_pred, y_test)\n",
    "\n",
    "print \"completeness\", completeness\n",
    "print \"contamination\", contamination\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute the decision boundary\n",
    "svm = y_class[1]\n",
    "w = svm.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "yy = np.linspace(-0.1, 0.4)\n",
    "xx = a * yy - svm.intercept_[0] / w[1]\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(xx, yy, '-k')\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.binary, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "ax.set_xlim(0.7, 1.35)\n",
    "ax.set_ylim(-0.15, 0.4)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some comments on these results:\n",
    "\n",
    "- The median of a distribution is unaffected by large perturbations of outlying points, as long as those perturbations do not cross the boundary.\n",
    "- In the same way, once the support vectors are determined, changes to the positions or numbers of points beyond the margin will not change the decision boundary.  For this reason, SVM can be a very powerful tool for discriminative classification.\n",
    "- This is why there is a high completeness compared to the other methods: it does not matter that the background sources outnumber the RR Lyrae stars by a factor of $\\sim$200 to 1. It simply determines the best boundary between the small RR Lyrae clump and the large background clump.\n",
    "- This completeness, however, comes at the cost of a relatively large contamination level.\n",
    "\n",
    "Note that:\n",
    "- SVM is not scale invariant so it often worth rescaling the data to [0,1] or to whiten it to have a mean of 0 and variance 1 (remember to do this to the test data as well!)\n",
    "- The data don't need to be separable (we can put a constraint in minimizing the number of \"failures\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kernel Methods\n",
    "If the contamination is driven by non-linear effects (which isn't the case here), it may be worth implementing a non-linear decision boundary.  As before, we do that by *kernelization*.  \n",
    "\n",
    "Go to [Scikit-Learn SVM](http://scikit-learn.org/stable/modules/svm.html) and see if you can figure out how to implement SVC with the 'rbf' kernel  (As in Figure 9.11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take a quick look at an example where the data are not linearly separable and where kernelization really makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plot_svc_decision_function(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we can make a transform of the data to make it linearly separable, for example with a simple **radial basis function** as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Transform X using a radial basis function\n",
    "r = np.exp(-(X[:, 0] ** 2 + X[:, 1] ** 2))\n",
    "\n",
    "from IPython.html.widgets import interact\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='spring')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "plot_3D()\n",
    "# GTR: Or even fancier with \n",
    "# interact(plot_3D, elev=[-90, 90], azip=(-180, 180));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can trivially separate these populations as shown below!  \n",
    "GTR: Something not quite right here, but you get the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "A [**decision tree**](https://en.wikipedia.org/wiki/Decision_tree) is similar to the process of classification that you might do by hand: define some criteria to separate the sample into 2 groups (not necessarily equal).  Then take those sub-groups and do it again.  Keep going until you reach a stopping point such as not having a minimum number of objects to split again.  In short, we have done a hierarchical application of decision boundaries.\n",
    "\n",
    "The tree structure is as follows:\n",
    "- top node contains the entire data set\n",
    "- at each branch the data are subdivided into two child nodes \n",
    "- split is based on a predefined decision boundary (usually axis aligned)\n",
    "- splitting repeats, recursively, until we reach a predefined stopping criteria \n",
    "\n",
    "Below is a simple example of a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Source: Jake VanderPlas, https://github.com/LocalGroupAstrostatistics2015/MachineLearning/blob/master/fig_code/figures.py\n",
    "def plot_example_decision_tree():\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = fig.add_axes([0, 0, 0.8, 1], frameon=False, xticks=[], yticks=[])\n",
    "    ax.set_title('Example Decision Tree: Animal Classification', size=24)\n",
    "\n",
    "    def text(ax, x, y, t, size=20, **kwargs):\n",
    "        ax.text(x, y, t,\n",
    "                ha='center', va='center', size=size,\n",
    "                bbox=dict(boxstyle='round', ec='k', fc='w'), **kwargs)\n",
    "\n",
    "    text(ax, 0.5, 0.9, \"How big is\\nthe animal?\", 20)\n",
    "    text(ax, 0.3, 0.6, \"Does the animal\\nhave horns?\", 18)\n",
    "    text(ax, 0.7, 0.6, \"Does the animal\\nhave two legs?\", 18)\n",
    "    text(ax, 0.12, 0.3, \"Are the horns\\nlonger than 10cm?\", 14)\n",
    "    text(ax, 0.38, 0.3, \"Is the animal\\nwearing a collar?\", 14)\n",
    "    text(ax, 0.62, 0.3, \"Does the animal\\nhave wings?\", 14)\n",
    "    text(ax, 0.88, 0.3, \"Does the animal\\nhave a tail?\", 14)\n",
    "\n",
    "    text(ax, 0.4, 0.75, \"> 1m\", 12, alpha=0.4)\n",
    "    text(ax, 0.6, 0.75, \"< 1m\", 12, alpha=0.4)\n",
    "\n",
    "    text(ax, 0.21, 0.45, \"yes\", 12, alpha=0.4)\n",
    "    text(ax, 0.34, 0.45, \"no\", 12, alpha=0.4)\n",
    "\n",
    "    text(ax, 0.66, 0.45, \"yes\", 12, alpha=0.4)\n",
    "    text(ax, 0.79, 0.45, \"no\", 12, alpha=0.4)\n",
    "\n",
    "    ax.plot([0.3, 0.5, 0.7], [0.6, 0.9, 0.6], '-k')\n",
    "    ax.plot([0.12, 0.3, 0.38], [0.3, 0.6, 0.3], '-k')\n",
    "    ax.plot([0.62, 0.7, 0.88], [0.3, 0.6, 0.3], '-k')\n",
    "    ax.plot([0.0, 0.12, 0.20], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.plot([0.28, 0.38, 0.48], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.plot([0.52, 0.62, 0.72], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.plot([0.8, 0.88, 1.0], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.axis([0, 1, 0, 1])\n",
    "\n",
    "plot_example_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The \"leaf (terminal) nodes\" record the fraction of points that have one classification or the other\n",
    "\n",
    "Application of the tree to classification is simple (a series of binary decisions). The  fraction of points from the training set classified as one class or the other (in the leaf node) defines the class associated with that leaf node.\n",
    "\n",
    "The binary splitting makes this extremely efficient. Tthe trick is to ask the *right* questions.\n",
    "So, decision trees are simple to interpret (just a set of questions).\n",
    "\n",
    "Scikit-learn implements the [`DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "dtree = DecisionTreeClassifier(max_depth=6)\n",
    "dtree.fit(X,y)\n",
    "y_pred = dtree.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example with our data set of RR Lyrae stars shows that it has moderately good completenees and contamination, but that, for this data set, it is not the optimal choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 9.13\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "\n",
    "N_tot = len(y)\n",
    "N_stars = np.sum(y == 0)\n",
    "N_rrlyrae = N_tot - N_stars\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rrlyrae\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Fit Decision tree\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "\n",
    "y_class = []\n",
    "y_pred = []\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "depths = [7, 12]\n",
    "\n",
    "for depth in depths:\n",
    "    y_class.append([])\n",
    "    y_pred.append([])\n",
    "    for nc in Ncolors:\n",
    "        dt = DecisionTreeClassifier(random_state=0, max_depth=depth,\n",
    "                                     criterion='entropy')\n",
    "        dt.fit(X_train[:, :nc], y_train)\n",
    "        y_pred[-1].append(dt.predict(X_test[:, :nc]))\n",
    "        y_class[-1].append(dt)\n",
    "\n",
    "completeness, contamination = completeness_contamination(y_pred, y_test)\n",
    "\n",
    "print \"completeness\", completeness\n",
    "print \"contamination\", contamination\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute the decision boundary\n",
    "\n",
    "dt = y_class[1][1]\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 101),\n",
    "                     np.linspace(ylim[0], ylim[1], 101))\n",
    "#GTR Whoa!?  Why is this yy, xx and not xx, yy????\n",
    "#Ah, because the plot reverses the usual python order\n",
    "xystack = np.vstack([yy.ravel(),xx.ravel()])\n",
    "Xgrid = xystack.T\n",
    "\n",
    "Z = dt.predict(Xgrid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.binary, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "ax.text(0.02, 0.02, \"depth = %i\" % depths[1],\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness[0], 'o-k', ms=6, label=\"depth=%i\" % depths[0])\n",
    "ax.plot(Ncolors, completeness[1], '^--k', ms=6, label=\"depth=%i\" % depths[1])\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination[0], 'o-k', ms=6, label=\"depth=%i\" % depths[0])\n",
    "ax.plot(Ncolors, contamination[1], '^--k', ms=6, label=\"depth=%i\" % depths[1])\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.0, 0.79))\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Splitting Criteria\n",
    "\n",
    "Now let's talk about the best ways to split the data.  This is actually a really hard problem, which you can read about more in Ivezic $\\S$ 9.7.1.  \n",
    "\n",
    "One way is to use the information content or entropy, $E(x)$, of the data\n",
    "\n",
    "$$ E(x) = -\\sum_i p_i(x) \\ln (p_i(x)),$$\n",
    "\n",
    "where $i$ is the class and $p_i(x)$ is the probability of that class\n",
    "given the training data. \n",
    "\n",
    "Another commonly used \"loss function\" (especially for categorical classification) is the Gini coefficient:\n",
    "$$ G = \\sum_i^k p_i(1-p_i).$$\n",
    "\n",
    "It essentially estimates the probability of incorrect classification by choosing both a point and (separately) a class randomly from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviously in constructing a decision treee, if your choice of stopping criteria is too loose, further splitting just ends up adding noise.  So here is an example using cross-validation in order to optimize the depth of the tree (and to avoid overfitting).  \n",
    "\n",
    "Note that here we aren't classifying the objects into discrete categories, rather we are classifying them into a continuous category.  That is, we are doing regression.  In this particular case, we are using the colors of galaxies in order to predict their redshifts (distances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 9.14\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch data and prepare it for the computation\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 separate points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the cross-validation scores for several tree depths\n",
    "depth = np.arange(1, 21)\n",
    "rms_test = np.zeros(len(depth))\n",
    "rms_train = np.zeros(len(depth))\n",
    "i_best = 0\n",
    "z_fit_best = None\n",
    "\n",
    "for i, d in enumerate(depth):\n",
    "    clf = DecisionTreeRegressor(max_depth=d, random_state=0)\n",
    "    clf.fit(mag_train, z_train)\n",
    "\n",
    "    z_fit_train = clf.predict(mag_train)\n",
    "    z_fit_test = clf.predict(mag_test)\n",
    "    rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "    rms_test[i] = np.mean(np.sqrt((z_fit_test - z_test) ** 2))\n",
    "\n",
    "    if rms_test[i] <= rms_test[i_best]:\n",
    "        i_best = i\n",
    "        z_fit_best = z_fit_test\n",
    "\n",
    "best_depth = depth[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# first panel: cross-validation\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(depth, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(depth, rms_train, '--k', label='training set')\n",
    "ax.set_xlabel('depth of tree')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "ax.set_xlim(0, 21)\n",
    "ax.set_ylim(0.009,  0.04)\n",
    "ax.legend(loc=1)\n",
    "\n",
    "# second panel: best-fit results\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(z_test, z_fit_best, s=1, lw=0, c='k')\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.04, 0.96, \"depth = %i\\nrms = %.3f\" % (best_depth, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "ax.set_xlabel(r'$z_{\\rm true}$')\n",
    "ax.set_ylabel(r'$z_{\\rm fit}$')\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's doing the Cross Validation by hand, let's try it automatically using data like the first example that we started with today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=500, centers=3,\n",
    "                  random_state=0, cluster_std=1.50)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "drange = np.arange(1,21)\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid={'max_depth': drange}, cv=5)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"best parameter choice:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(# Complete\n",
    "dt.fit(# Complete\n",
    "\n",
    "xlim = (-4, 8)\n",
    "ylim = (-6, 10)\n",
    "\n",
    "xx, yy = np.meshgrid( # Complete\n",
    "xystack = np.vstack( # Complete\n",
    "Xgrid = # Complete\n",
    "\n",
    "Z = dt.predict(# Complete\n",
    "Z = Z.reshape(# Complete\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.spring, zorder=2)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "You may have noticed that each of the classification methods that we have described so far has its strengths and weaknesses.  You might wonder if we could gain something by some sort of averaging of weighted \"voting\".  Such a process is what we call *ensemble learning*.  We'll discuss two such processes: [**bagging**](https://en.wikipedia.org/wiki/Bootstrap_aggregating) and [**random forests**](https://en.wikipedia.org/wiki/Random_forest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bagging\n",
    "\n",
    "Bagging (short for *bootstrap aggregation*--a name, unlike SVM, which actually makes some sense) can significantly improve the performance of decision trees.  In short, bagging averages the predictive results of a series of bootstrap samples.\n",
    "\n",
    "Bootstrap is one of the many important things that we skipped over earlier in the course.  It is just another way of dividing the data set for testing.  \n",
    "\n",
    "Instead of splitting the sample into training and test sets that do not overlap, bootstrap says to draw from the observed data set with replacement.  So we select indices $j$ from the range $i=1,\\ldots,N$ and this is our new sample.  Some indices, $i$, will be repeated and we do this $B$ times.\n",
    "\n",
    "For a sample of $N$ points in a training set, bagging generates $B$ equally sized bootstrap samples from which to estimate the function $f_i(x)$. The final estimator for $\\hat{y}$, defined by bagging, is then\n",
    "$$\\hat{y} = f(x) = \\frac{1}{B} \\sum_i^B f_i(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forests\n",
    "Random forests extend bagging by generating decision trees from the bootstrap samples.  A interesting aspect of random forests is that the features on which to generate the tree are selected at random from the full set of features in the data (the number of features selected per split level is typically the square root of the total number of attributes, $\\sqrt{D}$).  The final classification from the random forest is based on the averaging of the classifications of each of the individual decision trees.  So, you can literally give it the kitchen sink (including attributes that you might not otherwise think would be useful for classification).\n",
    "\n",
    "Random forests help to overcome some of the limitations of decision trees.\n",
    "\n",
    "As before, cross-validation can be used to determine the optimal depth.  Generally the number of trees, $n$, that are chosen is the number at which the cross-validation error plateaus.\n",
    "\n",
    "Below we give the same example as above for estimation of galaxy redshifts, where Scikit-Learn's [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) call looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "ranfor = RandomForestClassifier(10)\n",
    "ranfor.fit(X,y)\n",
    "y_pred = ranfor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 9.15\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch and prepare the data\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 distinct points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "def compute_photoz_forest(depth):\n",
    "    rms_test = np.zeros(len(depth))\n",
    "    rms_train = np.zeros(len(depth))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, d in enumerate(depth):\n",
    "        clf = RandomForestRegressor(n_estimators=10,\n",
    "                                    max_depth=d, random_state=0)\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best\n",
    "\n",
    "\n",
    "depth = np.arange(1, 21)\n",
    "rms_test, rms_train, i_best, z_fit_best = compute_photoz_forest(depth)\n",
    "best_depth = depth[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(depth, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(depth, rms_train, '--k', label='training set')\n",
    "ax.legend(loc=1)\n",
    "\n",
    "ax.set_xlabel('depth of tree')\n",
    "ax.set_ylabel('rms error')\n",
    "\n",
    "ax.set_xlim(0, 21)\n",
    "ax.set_ylim(0.009,  0.04)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(z_test, z_fit_best, s=1, lw=0, c='k')\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.03, 0.97, \"depth = %i\\nrms = %.3f\" % (best_depth, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$z_{\\rm true}$')\n",
    "ax.set_ylabel(r'$z_{\\rm fit}$')\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is an ensemble approach motivated by the idea that combining many weak classifiers can result in an improved classification. Boosting creates models that attempt to correct the errors of the ensemble so far. At the heart of boosting is the idea that we reweight the data based on how incorrectly the data were classified in the previous iteration.\n",
    "\n",
    "We run the classification multiple times and each time reweight the data based on the previous performance of the classifier. At the end of this procedure we allow the classifiers to vote on the final classification. The most popular form of boosting is that of adaptive boosting.  In this case we take a weak classifier, $h(x)$, and create a strong classifier, $f(x)$, as\n",
    "$$ f(x) = \\sum_m^B\\theta_m h_m(x),$$\n",
    "where $m$ is the number of iterations and $\\theta_m$ is the weight of the classifier in each iteration.  \n",
    "\n",
    "If we chose $\\theta_m=1/B$, then we'd essentially have bagging.  For boosting the idea is to increase the weight of the misclassified data in each step.\n",
    "\n",
    "A fundamental limitation of the boosted decision tree is the computation time for large data sets (they rely on a chain of classifiers which are each dependent on the last), whereas random forests can be easily parallelized.\n",
    "\n",
    "The example given below is actually Scikit-Learn's [`GradientBoostingClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) where we approximate the steepest descent criterion after each simple evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "gradboost = GradientBoostingClassifier()\n",
    "gradboost.fit(X,y)\n",
    "y_pred = gradboost.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 9.16\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch and prepare the data\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 distinct points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "def compute_photoz_forest(N_boosts):\n",
    "    rms_test = np.zeros(len(N_boosts))\n",
    "    rms_train = np.zeros(len(N_boosts))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, Nb in enumerate(N_boosts):\n",
    "        try:\n",
    "            # older versions of scikit-learn\n",
    "            clf = GradientBoostingRegressor(n_estimators=Nb, learn_rate=0.1,\n",
    "                                            max_depth=3, max_features='sqrt', random_state=0)\n",
    "        except TypeError:\n",
    "            clf = GradientBoostingRegressor(n_estimators=Nb, learning_rate=0.1,\n",
    "                                            max_depth=3, max_features='sqrt', random_state=0)\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best\n",
    "\n",
    "N_boosts = (10, 100, 200, 300, 400, 500)\n",
    "rms_test, rms_train, i_best, z_fit_best = compute_photoz_forest(N_boosts)\n",
    "best_N = N_boosts[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(N_boosts, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(N_boosts, rms_train, '--k', label='training set')\n",
    "ax.legend(loc=1)\n",
    "\n",
    "ax.set_xlabel('number of boosts')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.set_xlim(0, 510)\n",
    "ax.set_ylim(0.009,  0.032)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "ax.text(0.03, 0.03, \"Tree depth: 3\",\n",
    "        ha='left', va='bottom', transform=ax.transAxes)\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(z_test, z_fit_best, s=1, lw=0, c='k')\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.04, 0.96, \"N = %i\\nrms = %.3f\" % (best_N, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$z_{\\rm true}$')\n",
    "ax.set_ylabel(r'$z_{\\rm fit}$')\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alright, alright but what the @#%! should I use?\n",
    "\n",
    "A convenient cop-out: no single model can be known in advance to be the best classifier!\n",
    "\n",
    "In general the level of accuracy increases for parametric models as:\n",
    "- <b>naive Bayes</b>, \n",
    "- linear discriminant analysis (LDA),\n",
    "- logistic regression, \n",
    "- linear support vector machines, \n",
    "- quadratic discriminant analysis (QDA),\n",
    "- linear ensembles of linear models. \n",
    "\n",
    "For non-parametric models accuracy increases as:\n",
    "- decision trees\n",
    "- $K$-nearest-neighbor, \n",
    "- neural networks\n",
    "- kernel discriminant analysis,\n",
    "- <b> kernelized support vector machines</b>\n",
    "- <b> random forests</b>\n",
    "- boosting\n",
    "\n",
    "See also Ivezic, Table 9.1.\n",
    "\n",
    "Naive Bayes and its variants are by far the easiest to compute. Linear support vector machines are more expensive, though several fast algorithms exist. Random forests can be easily parallelized. \n",
    "\n",
    "We saw before that Scikit-learn has tools for computing ROC curves, which is implemented as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "\n",
    "gnb = GaussianNB().fit(X,y)\n",
    "y_prob = gnb.predict_proba(X)\n",
    "\n",
    "# Compute precision/recall curve\n",
    "pr, re, thresh = metrics.precision_recall_curve(y, y_prob[:,0])\n",
    "\n",
    "# Compute ROC curve\n",
    "tpr, fpr, thresh = metrics.roc_curve(y, y_prob[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's remember what they had to say:\n",
    "\n",
    "![Ivezic, Figure 9.17](http://www.astroml.org/_images/fig_ROC_curve_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's an example with a different data set.  Here we are trying to distinguish quasars (in black) from stars (in grey).  Add a precision recall plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, 9.18\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.utils import split_samples\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve # Complete\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from astroML.classification import GMMBayes\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch data and split into training and test samples\n",
    "from astroML.datasets import fetch_dr7_quasar\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "\n",
    "quasars = fetch_dr7_quasar()\n",
    "stars = fetch_sdss_sspp()\n",
    "\n",
    "# Truncate data for speed\n",
    "quasars = quasars[::5]\n",
    "stars = stars[::5]\n",
    "\n",
    "# stack colors into matrix X\n",
    "Nqso = len(quasars)\n",
    "Nstars = len(stars)\n",
    "X = np.empty((Nqso + Nstars, 4), dtype=float)\n",
    "\n",
    "X[:Nqso, 0] = quasars['mag_u'] - quasars['mag_g']\n",
    "X[:Nqso, 1] = quasars['mag_g'] - quasars['mag_r']\n",
    "X[:Nqso, 2] = quasars['mag_r'] - quasars['mag_i']\n",
    "X[:Nqso, 3] = quasars['mag_i'] - quasars['mag_z']\n",
    "\n",
    "X[Nqso:, 0] = stars['upsf'] - stars['gpsf']\n",
    "X[Nqso:, 1] = stars['gpsf'] - stars['rpsf']\n",
    "X[Nqso:, 2] = stars['rpsf'] - stars['ipsf']\n",
    "X[Nqso:, 3] = stars['ipsf'] - stars['zpsf']\n",
    "\n",
    "y = np.zeros(Nqso + Nstars, dtype=int)\n",
    "y[:Nqso] = 1\n",
    "\n",
    "# split into training and test sets\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.9, 0.1],\n",
    "                                                     random_state=0)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute fits for all the classifiers\n",
    "def compute_results(*args):\n",
    "    names = []\n",
    "    probs = []\n",
    "\n",
    "    for classifier, kwargs in args:\n",
    "        print classifier.__name__\n",
    "        model = classifier(**kwargs)\n",
    "        model.fit(X, y)\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "\n",
    "        names.append(classifier.__name__)\n",
    "        probs.append(y_prob[:, 1])\n",
    "\n",
    "    return names, probs\n",
    "\n",
    "LRclass_weight = dict([(i, np.sum(y_train == i)) for i in (0, 1)])\n",
    "\n",
    "names, probs = compute_results((GaussianNB, {}),\n",
    "                               (LDA, {}),\n",
    "                               (QDA, {}),\n",
    "                               (LogisticRegression,\n",
    "                                dict(class_weight=LRclass_weight)),\n",
    "                               (KNeighborsClassifier,\n",
    "                                dict(n_neighbors=10)),\n",
    "                               (DecisionTreeClassifier,\n",
    "                                dict(random_state=0, max_depth=12,\n",
    "                                     criterion='entropy')),\n",
    "                               (GaussianMixture, dict(n_components=3, tol=1E-5,\n",
    "                                               covariance_type='full')))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot results\n",
    "fig = plt.figure(figsize=(9, 4))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.15, top=0.9, wspace=0.25)\n",
    "\n",
    "# First axis shows the data\n",
    "ax1 = fig.add_subplot(131)\n",
    "im = ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=4,\n",
    "                 linewidths=0, edgecolors='none',\n",
    "                 cmap=plt.cm.binary)\n",
    "im.set_clim(-0.5, 1)\n",
    "ax1.set_xlim(-0.5, 3.0)\n",
    "ax1.set_ylim(-0.3, 1.4)\n",
    "ax1.set_xlabel('$u - g$')\n",
    "ax1.set_ylabel('$g - r$')\n",
    "\n",
    "labels = dict(GaussianNB='GNB',\n",
    "              LinearDiscriminantAnalysis='LDA',\n",
    "              QuadraticDiscriminantAnalysis='QDA',\n",
    "              KNeighborsClassifier='KNN',\n",
    "              DecisionTreeClassifier='DT',\n",
    "              GaussianMixture='GMMB',\n",
    "              LogisticRegression='LR')\n",
    "\n",
    "# Second axis shows the ROC curves\n",
    "ax2 = fig.add_subplot(132)\n",
    "for name, y_prob in zip(names, probs):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "    fpr = np.concatenate([[0], fpr])\n",
    "    tpr = np.concatenate([[0], tpr])\n",
    "    \n",
    "    ax2.plot(fpr, tpr, label=labels[name])\n",
    "\n",
    "ax2.legend(loc=4)\n",
    "ax2.set_xlabel('false positive rate')\n",
    "ax2.set_ylabel('true positive rate')\n",
    "ax2.set_xlim(0, 0.15)\n",
    "ax2.set_ylim(0.6, 1.01)\n",
    "ax2.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Curiously GMMBayes went from being one of the best to one of the worst after I changed from using the deprecated GMMBayes to GaussianMixture.  So, it is likely that the current input parameters are not optimal for that."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
