# PHYS_T480

This is the repository for PHYS T480/580 "Big Data Physics: Methods of Machine Learning" at Drexel University, taught by Prof. Gordon Richards.  The course syllabus can be found at http://www.physics.drexel.edu/~gtr/teaching/physT480/

**This is an archived class from Fall 2016.  The repository for the current class (Fall 2018) is https://github.com/gtrichards/PHYS_T480_F18/ .

The course is a series of jupyter notebooks, where I have drawn heavily from resources from the following people/places:

Jake Vanderplas (University of Washington) -- one of the primary code developers of scikit-learn and astroML.  I draw a lot from https://github.com/jakevdp/ESAC-stats-2014, but you can find a lot more from him too: https://github.com/jakevdp/.

Zeljko Ivezic (University of Washington) -- the lead author of the textbook that we use (http://press.princeton.edu/titles/10159.html)

Andy Connolly (University of Washington), particularly http://cadence.lsst.org/introAstroML/

Karen Leighly (University of Oklahoma), particularly http://http://seminar.ouml.org/

Adam Miller (Northwestern University), particularly https://github.com/LSSTC-DSFP/LSSTC-DSFP-Sessions/

Jo Bovy (University of Toronto), particularly http://astro.utoronto.ca/~bovy/teaching.html

Thomas Wiecki, particularly http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/

My thanks also to Maher Harb (Drexel University), Liam Coatman (Cambridge), Nathalie Thibert (UWO), and Kevin Footer (Deloitte).

I have tried to be careful about properly attributing anything drawn from these resources, but if it isn't clear where something comes from, it is probably there.
Others are welcome to draw from here for their own Machine Learning courses.  Please send any corrections to gtr@physics.drexel.edu.

If you have any interest in using these materials for your own Machine Learning course, please e-mail me and I'll send you my post lecture notes about what worked, what didn't, what took too long, what didn't take long enough -- basically what I would change for next time.

## Schedule

Lecture 1 (9/20): Motivation.ipynb and InitialSetup.ipynb

Lecture 2 (9/22): HistogramExample.ipynb

Lecture 3 (9/27): BasicStats.ipynb

Lecture 4 (9/29): BasicStats2.ipynb

Lecture 5 (10/4): Inference.ipynb

Lecture 6 (10/6): Inference2.ipynb

Lecture 7 (10/11): Scikit-Learn-Intro.ipynb

Lecture 8 (10/13): Ditto

Lecture 9 (10/18): KernelDensityEstimation.ipynb and NearestNeighbor.ipynb

Lecture 10 (10/20): MixtureModel.ipynb and Clustering.ipynb

Lecture 11 (10/25): DimensionReduction.ipynb

Lecture 12 (10/27): NonlinearDimensionReduction.ipynb

Lecture 13 (11/1): Regression.ipynb

Lecture 14 (11/3): Regression2.ipynb

Lecture 15 (11/8): Classification.ipynb

Lecture 16 (11/10): Classification2.ipynb

Lecture 17 (11/15): TimeSeries.ipynb

Lecture 18 (11/17): TimeSeries2.ipynb and NeuralNetworks.ipynb

Lecture 19 (11/22): Project Presentations

Lecture 20 (11/29): Project Presentations

Lecture 21 (12/1): Project Presentations
