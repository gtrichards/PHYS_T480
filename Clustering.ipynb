{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "G. Richards (2016), based on materials from Ivezic, Vanderplas, Leighly, and Nathalie Thibert.\n",
    "\n",
    "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) algorithms attempt to group together like objects in a data set.  This process allows us to put new objects into the resulting classes and to identify rare objects that do not fit any particular mold.  Clustering is inherently an \"unsupervised\" process as we do not know the classification of the objects.  Since we have no metric for determining when we are right, it is a bit of a black art, but it also can be very powerful.  Scikit-Learn's clustering suite is summarized at [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $K$-Means Clustering\n",
    "\n",
    "We start with [\"$K$-means\" clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), which is one of the simplest methods.  \"$K$-means\" seeks to minimize the following\n",
    "\n",
    "$$\\sum_{k=1}^{K}\\sum_{i\\in C_k}||x_i - \\mu_k||^2$$\n",
    "\n",
    "where $\\mu_k = \\frac{1}{N_k}\\sum_{i\\in C_k} x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This says to\n",
    "  * Take every object in class $C_k$ (as determined by which centroid it is closest to, specifically $C_k(x_i) = \\arg \\min_k ||x_i-\\mu_k||)$\n",
    "  * Compute the mean of the objects in that class\n",
    "  * Subtract that mean from each member of that class and square the norm\n",
    "  * Do that for each class and sum\n",
    "  * Shift the centroids of the *pre-determined* number of classes until this sum is minimized\n",
    "  * Do this multiple times with different starting centroids and take the result with the minimum sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A typical call will look something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = np.random.normal(size=(1000,2)) #1000 points in 2D\n",
    "clf = KMeans(n_clusters=3) #Try 3 clusters to start with\n",
    "clf.fit(X)\n",
    "centers=clf.cluster_centers_ #location of the clusters\n",
    "labels=clf.predict(X) #labels for each of the points\n",
    "\n",
    "# To get some information on these try:\n",
    "# KMeans?\n",
    "# help(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is an example with the same data that we used for GMM.  Note how the background shifts the centroids from what you might expect.  So, the mixture model might work better in this case.\n",
    "\n",
    "However, one might consider runnig the K-means algorithm in order to find a suitable initialization for GMM.  Do you think that might have helped with our mis-behaving GMM example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Based on Ivezic, Figure 6.13\n",
    "# GTR fixed bug by changing preprocessing.Scaler() to preprocessing.StandardScaler()\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "# Remove some outliers\n",
    "data = data[~((data['alphFe'] > 0.4) & (data['FeH'] > -0.3))]\n",
    "\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a 2D histogram  of the input\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'], 50)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the KMeans clustering\n",
    "n_clusters = 4\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "clf = KMeans(n_clusters)\n",
    "clf.fit(scaler.fit_transform(X))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Visualize the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# plot density\n",
    "ax = plt.axes()\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "# plot cluster centers\n",
    "cluster_centers = scaler.inverse_transform(clf.cluster_centers_)\n",
    "ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
    "           s=40, c='w', edgecolors='k')\n",
    "\n",
    "# plot cluster boundaries\n",
    "# using a fancy way to compute the average of the left and right edges of each bin\n",
    "FeH_centers = 0.5 * (FeH_bins[1:] + FeH_bins[:-1])\n",
    "alphFe_centers = 0.5 * (alphFe_bins[1:] + alphFe_bins[:-1])\n",
    "\n",
    "Xgrid = np.meshgrid(FeH_centers, alphFe_centers)\n",
    "Xgrid = np.array(Xgrid).reshape((2, 50 * 50)).T\n",
    "\n",
    "H = clf.predict(scaler.transform(Xgrid)).reshape((50, 50))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    Hcp = H.copy()\n",
    "    flag = (Hcp == i)\n",
    "    Hcp[flag] = 1\n",
    "    Hcp[~flag] = 0\n",
    "\n",
    "    ax.contour(FeH_centers, alphFe_centers, Hcp, [-0.5, 0.5],\n",
    "               linewidths=1, colors='k')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A few things to note\n",
    "* We scaled the data before running K-Means\n",
    "* We had to *un*scale the data to plot the centers\n",
    "* Plotting the cluster boundaries is not straightforward, but this gives you an example to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another clustering algorithm is [Mean Shift](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html).  It finds local modes in the peaks of a nonparametric density estimate (like KDE) and identifies each data point with the closest peak.  \n",
    "\n",
    "Its call looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "X = np.random.normal(size=(1000,2))\n",
    "ms = MeanShift(bandwidth=1.0) #can also specify with no bandwidth\n",
    "ms.fit(X)\n",
    "centers = ms.cluster_centers_\n",
    "labels = ms.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try replacing the $K$-means algorithm in the figure above with Mean shift.  Call it with `bin_seeding=True` to speed things up a bit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is an example using some real data provided by Prof. Cruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the next few cells\n",
    "from astropy.table import Table\n",
    "t = Table.read('data/cruz_all_dist.dat', format=\"ascii\")\n",
    "\n",
    "# Just something that you should know that you can do\n",
    "t[::10000].show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Turn these data into a properly formatted Scikit-Learn array\n",
    "X = np.vstack([ t['col2'], t['col3'], t['col4'], t['col5'] ]).T\n",
    "print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Project onto 2 axes with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) # 2 components\n",
    "pca.fit(X) # Do the fitting\n",
    "\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], marker=\".\", color='k', edgecolors='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the KMeans clustering\n",
    "n_clusters = 6\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "clf = KMeans(n_clusters)\n",
    "clf.fit(scaler.fit_transform(X_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make some plots\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# Compute a 2D histogram  of the input\n",
    "H, xedges, yedges = np.histogram2d(X_reduced[:,0], X_reduced[:,1], 50)\n",
    "\n",
    "# plot density\n",
    "ax = plt.axes()\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[xedges[0], xedges[-1],\n",
    "                  yedges[0], yedges[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "# plot cluster centers\n",
    "cluster_centers = scaler.inverse_transform(clf.cluster_centers_)\n",
    "ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
    "           s=40, c='w', edgecolors='k')\n",
    "\n",
    "# plot cluster boundaries\n",
    "x_centers = 0.5 * (xedges[1:] + xedges[:-1])\n",
    "y_centers = 0.5 * (yedges[1:] + yedges[:-1])\n",
    "\n",
    "Xgrid = np.meshgrid(x_centers, y_centers)\n",
    "Xgrid = np.array(Xgrid).reshape((2, 50 * 50)).T\n",
    "\n",
    "H = clf.predict(scaler.transform(Xgrid)).reshape((50, 50))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    Hcp = H.copy()\n",
    "    flag = (Hcp == i)\n",
    "    Hcp[flag] = 1\n",
    "    Hcp[~flag] = 0\n",
    "\n",
    "    ax.contour(x_centers, y_centers, Hcp, [-0.5, 0.5],\n",
    "               linewidths=1, colors='k')\n",
    "\n",
    "#ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(xedges[0], xedges[-1])\n",
    "ax.set_ylim(yedges[0], yedges[-1])\n",
    "\n",
    "ax.set_xlabel('Eigenvalue 1')\n",
    "ax.set_ylabel('Eigenvalue 2')\n",
    "\n",
    "plt.savefig('cruz.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "In [Hierarchical Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), we don't specify the number of clusters ahead of time, we start with $N$ clusters representing each data point.  Then the most similar clusters are joined together, the process repeating until some threshhold is reached.  Actually the process can go in the other direction as well.  What results is what is called a *dendrogram*, an example of which is shown below.\n",
    "\n",
    "![](https://onlinecourses.science.psu.edu/stat505/sites/onlinecourses.science.psu.edu.stat505/files/lesson12/dendogram_example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Clusters are merged at each step according to which are \"nearest\" to each other---where the definition of nearest needs to be specified.  A typical choice results in what is called a \"minimum spanning tree\" (which can be quite slow for large data sets).  Some threshhold needs to be specified to tell the process where to stop (e.g., we are going to treat the green and red objects in the example above as separate clusters).  \n",
    "\n",
    "Below is an example call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "X = np.random.random((1000,2))\n",
    "G = kneighbors_graph(X, n_neighbors=10, mode='distance')\n",
    "T = minimum_spanning_tree(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK, but that's all that the book give us.  There is nothing about what to do with `G` and `T`.  So, instead I'm going to show you a really cool example from a colleague.  In this example Nathalie Thibert is taking spectroscopic data of a certain sub-class of quasars.  She is then grouping the objects into \"like\" bins using a hierarchical clustering algorithm.  The code below is based on the [scipy implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) and takes us through both the analysis and visualization of the data.  It makes use of the [Python Data Analysis Library (pandas)](http://pandas.pydata.org/) and ['pickled'](https://docs.python.org/2/library/pickle.html) \n",
    "data, both of which we have not talked about.\n",
    "\n",
    "For another detailed example of hierarchical clustering, see [https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%load code/thibert_cluster1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%load code/thibert_cluster2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the side and top dendrograms are the same data.  It is just that the 2-D visualization better lets us see what groups go together.\n",
    "\n",
    "I don't pretend to fully understand each step of this process, but the end result is really cool and I think that there is enough here to get you started if we were interested in trying to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary of Chapter 6 methods from Ivezic Table 6.1 \n",
    "\n",
    "|Method          |Accuracy|Interpretability|Simplicity|Speed|\n",
    "|----------------|--------|----------------|----------|-----|\n",
    "|K-nearest Neighbor| H | H | H | M |\n",
    "|Kernel Density Estimation| H | H | H | H |\n",
    "|Gaussian Mixture Models| H | M | M | M |\n",
    "|Extreme Deconvolution| H | H | M | M |\n",
    "||||||\n",
    "|K-Means| L | M | H | M |\n",
    "|Max-radius minimization| L | M | M | M |\n",
    "|Mean shift| M | H | H | M |\n",
    "|Hierarchical Clustering| H | L | L | L |"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
