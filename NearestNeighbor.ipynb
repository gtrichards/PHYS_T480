{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest-Neighbor Density Estimation\n",
    "\n",
    "Another very simple way to estimate the density of an $N$-dimensional distribution is to look to the nearest object (or the $K$ nearest objects) and compute their distances, $d_K$.  This is the [$K$-Nearest Neighbor](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm.  We'll see later that this is also a good method to use for classification.\n",
    "\n",
    "In this prescription, the density at a given point, $x$ is estimated as\n",
    "\n",
    "$$\\hat{f}_K(x) = \\frac{K}{V_D(d_K)}$$\n",
    "\n",
    "where $V_D(d)$ is given generically by $\\frac{2d^D\\pi^{D/2}}{D\\Gamma(D/2)}$ where $\\Gamma$ is the complete gamma function (Equation 3.58) and this formula reduces to the usual equations for area and volume in 2 and 3 dimensions, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can simplify this to \n",
    "$$\\hat{f}_K(x) = \\frac{C}{d_K^D}$$\n",
    "since the constant, $C$ can be evaluated at the end.\n",
    "\n",
    "This estimator is biased, so ideally we don't actually want the *nearest* neighbor, but rather we want something like the *5th* nearest neighbor (or larger). \n",
    "\n",
    "In fact, the error in the estimator can be reduced by considering *all* $K$ nearest neighbors:\n",
    "$$\\hat{f}_K(x) = \\frac{C}{\\sum_{i=1}^K d_i^D}$$\n",
    "\n",
    "See the [Scikit-Learn `neighbors` documentation](http://scikit-learn.org/stable/modules/neighbors.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ivezic, Figure 6.5 compares a Nearest Neighbor ($k=10$) with a KDE algorithm.  See what happens as you increase the number of neighbors used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Based on Ivezic, Figure 6.5\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from astroML.density_estimation import KNeighborsDensity\n",
    "from astroML.plotting import hist\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate our data: a mix of several Cauchy distributions\n",
    "#  this is the same data used in the Bayesian Blocks figure\n",
    "np.random.seed(0)\n",
    "N = 10000\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "true_pdf = lambda x: sum([f * stats.cauchy(mu, gamma).pdf(x) for (mu, gamma, f) in mu_gamma_f])\n",
    "x = np.concatenate([stats.cauchy(mu, gamma).rvs(int(f * N)) for (mu, gamma, f) in mu_gamma_f])\n",
    "np.random.shuffle(x)\n",
    "x = x[x > -10]\n",
    "x = x[x < 30]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "N = 5000\n",
    "k = 10\n",
    "\n",
    "xN = x[:N]\n",
    "t = np.linspace(-10, 30, 1000)\n",
    "\n",
    "# Compute density with KDE\n",
    "kde = KernelDensity(0.1, kernel='gaussian')\n",
    "kde.fit(xN[:, None])\n",
    "dens_kde = np.exp(kde.score_samples(t[:, None]))\n",
    "\n",
    "# Compute density with Bayesian nearest neighbors\n",
    "nbrs = KNeighborsDensity('bayesian', n_neighbors=k)\n",
    "nbrs.fit(xN[:, None])\n",
    "dens_nbrs = nbrs.eval(t[:, None]) / N\n",
    "\n",
    "# plot the results\n",
    "plt.plot(t, true_pdf(t), ':', color='black', zorder=3, label=\"Generating Distribution\")\n",
    "plt.plot(xN, -0.005 * np.ones(len(xN)), '|k')\n",
    "plt.plot(t, dens_nbrs, '-', lw=1.5, color='gray', zorder=2, label=\"Nearest Neighbors (k=%i)\" % k)\n",
    "plt.plot(t, dens_kde, '-', color='black', zorder=3, label=\"Kernel Density (h=0.1)\")\n",
    "\n",
    "# label the plot\n",
    "#plt.text(0.02, 0.95, \"%i points\" % N, ha='left', va='top', transform=ax.transAxes)\n",
    "plt.ylabel('$p(x)$')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(-0.01, 0.4001)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nearest-neighbors are both pretty simple and pretty powerful.  But you can imagine that they could also be really slow if you have either a lot of points or want to consider a lot of neighbors as  you have to compute all of the pairwise distances!  You can certainly do this \"brute force\" computation, but the use of **trees** speeds things up considerably.\n",
    "\n",
    "We haven't talked about **order notation** yet, but now is a good time to introduce it.  If we say that something \"goes as $\\mathscr{O}(N)$\", that means that $N$ operations are needed.  If is it $\\mathscr{O}(N^2)$, then $N\\times N$ operations are needed.\n",
    "\n",
    "If we have $N$ samples with $D$ features, then brute force nearest neighbor goes as $\\mathscr{O}(DN)$.  That can be a very large number of operations and make our code run slow.  So, can we do it faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trees\n",
    "\n",
    "It seems like you would be stuck computing all of the distances, but consider this:\n",
    "\n",
    "> if point A is very distant from point B, and point B is very close to point C, then we know that points A and C are very distant.\n",
    "\n",
    "So, we just have to compute the A-B and B-C distances; we don't actually need to compute A-C.\n",
    "\n",
    "We can take advantage of this using **trees**.  In 2-D we use a [**quad-tree**](https://en.wikipedia.org/wiki/Quadtree), which is illustrated in Ivezic, Figure 2.3 below.\n",
    "\n",
    "![Ivezic, Figure 2.3](http://www.astroml.org/_images/fig_quadtree_example_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is happening is that the data space gets broken down into smaller and smaller bins.  Then instead of computing the distances between each of the *objects*, we compute the distances between each of the *bins*.  We only compute the distances between objects if the bin distances are small enough to be considered further.  This process can speed up nearest neighbor finding considerably.  \n",
    "\n",
    "For a quad-tree we are specifically dividing the 2-D space into 4 equal \"nodes\" (children) until each box is either empty or has no more than some number of points.  The terminal nodes are called \"leaves\", thus the name \"tree\".  In 3-D we instead have **oct-trees**.  \n",
    "\n",
    "We can generalize this to $k$ dimensions but the scaling as $2^D$ quickly gets out of control and is called the [**curse of dimensionality**](https://en.wikipedia.org/wiki/Curse_of_dimensionality).   So for so-called [**$k$D-tree**](https://en.wikipedia.org/wiki/K-d_tree) we instead use *binary* trees where each node has 2 children instead of 4.   $k$D trees further split the *data* into two rather than the box into two.  A $k$D-tree is illustrated in Ivezic, Figure 2.4 below.\n",
    "\n",
    "![Ivezic, Figure 2.4](http://www.astroml.org/_images/fig_kdtree_example_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For $k$D trees no $D$-dimensional distances need to be computed, so once the tree is constructed, the nearest neighbor determination is only $\\mathscr{O}(D \\log N)$.\n",
    "\n",
    "In Scikit-Learn $k$D Trees are implemented in [http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree)\n",
    "\n",
    "As long as $D$ isn't *too* large ($\\lesssim 20$), this works well, but $k$D trees also suffer from the curse of dimensionality for large $D$ and go as $\\mathscr{O}(DN)$. In that case, we can use [**ball-trees**](https://en.wikipedia.org/wiki/Ball_tree) instead.  \n",
    "\n",
    "Instead of using Cartesian axes, ball-trees split the data into nesting hyper spheres.  This makes it more \"expensive\" to build the tree, but it makes finding the nearest neighbors faster $\\mathscr{O}(D \\log N)$.\n",
    "\n",
    "For more information on ball-trees in Scikit-Learn, see [http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree)\n",
    "\n",
    "An example from Ivezic is shown below\n",
    "![Ivezic, Figure 2.5](http://www.astroml.org/_images/fig_balltree_example_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating an evaluation grid\n",
    "\n",
    "Now let's look at nearest neighbors in 2-D.  Note that for 1-D we had to define the \"grid\" over which we evaluated the pdf and we did something like \n",
    "\n",
    "```t = np.linspace(-10, 30, 1000)```\n",
    "\n",
    "We need to do the same in 2-D.  That seems easy enough, right?  If $x$ runs from 0 to 10 and $y$ runs from 0 to 10 and we want to evaluate in steps of 1 in each direction, then we just need to build a grid that includes the points, $(0,0), (1,0), (0,1) \\dots (10,9), (9,10), (10,10)$. \n",
    "\n",
    "As far as I know there is no simple tool that does this.  But we can use [`np.meshgrid()`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html) to help us as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,10,11)\n",
    "y = np.linspace(0,10,11)\n",
    "print x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xv,yv = np.meshgrid(x,y)\n",
    "print xv\n",
    "print yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print xv.ravel()\n",
    "print yv.ravel()\n",
    "# Equivalent to flatten(), except for making a copy (or not) of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xystack = np.vstack([xv.ravel(),yv.ravel()])\n",
    "print xystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xgrid = xystack.T\n",
    "print Xgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that, while I said that there is no built in function for this, there are any number of ways to accomplish this!  For example, using `map`, `np.mgrid()`, transposes, etc.\n",
    "\n",
    "The code below accomplishes it in just one line, but I thought that it would make more sense if we broke it down like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Comparison of KDE and K-Nearest Neighbors \"smoothing\"\n",
    "# Based on Ivezic, Figure 6.4\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from astroML.datasets import fetch_great_wall\n",
    "from astroML.density_estimation import KDE, KNeighborsDensity\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch the great wall data\n",
    "X = fetch_great_wall()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create  the grid on which to evaluate the results\n",
    "Nx = 50\n",
    "Ny = 125\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Evaluate for several models\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx), np.linspace(ymin, ymax, Ny)))).T\n",
    "\n",
    "#print Xgrid\n",
    "\n",
    "kde = KDE(metric='gaussian', h=5)\n",
    "dens_KDE = kde.fit(X).eval(Xgrid).reshape((Ny, Nx))\n",
    "\n",
    "knn5 = KNeighborsDensity('bayesian', 5)\n",
    "dens_k5 = knn5.fit(X).eval(Xgrid).reshape((Ny, Nx))\n",
    "\n",
    "knn40 = KNeighborsDensity('bayesian', 40)\n",
    "dens_k40 = knn40.fit(X).eval(Xgrid).reshape((Ny, Nx))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(left=0.12, right=0.95, bottom=0.2, top=0.9,\n",
    "                    hspace=0.01, wspace=0.01)\n",
    "\n",
    "# First plot: scatter the points\n",
    "ax1 = plt.subplot(221, aspect='equal')\n",
    "ax1.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "ax1.text(0.95, 0.9, \"input\", ha='right', va='top',\n",
    "         transform=ax1.transAxes,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "# Second plot: KDE\n",
    "ax2 = plt.subplot(222, aspect='equal')\n",
    "ax2.imshow(dens_KDE.T, origin='lower', norm=LogNorm(),\n",
    "           extent=(ymin, ymax, xmin, xmax), cmap=plt.cm.binary)\n",
    "ax2.text(0.95, 0.9, \"KDE: Gaussian $(h=5)$\", ha='right', va='top',\n",
    "         transform=ax2.transAxes,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "# Third plot: KNN, k=5\n",
    "ax3 = plt.subplot(223, aspect='equal')\n",
    "ax3.imshow(dens_k5.T, origin='lower', norm=LogNorm(),\n",
    "           extent=(ymin, ymax, xmin, xmax), cmap=plt.cm.binary)\n",
    "ax3.text(0.95, 0.9, \"$k$-neighbors $(k=5)$\", ha='right', va='top',\n",
    "         transform=ax3.transAxes,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "# Fourth plot: KNN, k=40\n",
    "ax4 = plt.subplot(224, aspect='equal')\n",
    "ax4.imshow(dens_k40.T, origin='lower', norm=LogNorm(),\n",
    "           extent=(ymin, ymax, xmin, xmax), cmap=plt.cm.binary)\n",
    "ax4.text(0.95, 0.9, \"$k$-neighbors $(k=40)$\", ha='right', va='top',\n",
    "         transform=ax4.transAxes,\n",
    "         bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "for ax in [ax1, ax2, ax3, ax4]:\n",
    "    ax.set_xlim(ymin, ymax - 0.01)\n",
    "    ax.set_ylim(xmin, xmax)\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "for ax in [ax3, ax4]:\n",
    "    ax.set_xlabel('$y$ (Mpc)')\n",
    "\n",
    "for ax in [ax2, ax4]:\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "for ax in [ax1, ax3]:\n",
    "    ax.set_ylabel('$x$ (Mpc)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What the \"right\" answer is depends on what you want to do with the data.  \n",
    "\n",
    "Next time we'll talk about Gaussian Mixture Models and K-Means Clustering."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
