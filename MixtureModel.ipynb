{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Mixture Models (GMM)\n",
    "\n",
    "KDE centers each bin (or kernel rather) at each point.  In a [**mixture model**](https://en.wikipedia.org/wiki/Mixture_model) we don't use a kernel for each data point, but rather we fit for the *locations of the kernels*--in addition to the width.  So a mixture model is sort of a hybrid between an $N$-D histogram and KDE.   Using lots of kernels (maybe even more than the BIC score suggests) may make sense if you just want to provide an accurate description of the data (as in density estimation).  Using fewer kernels makes mixture models more like clustering, where the suggestion is still to use many kernels in order to divide the sample into real clusters and \"background\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussians are the most commonly used components for mixture models.  So, the pdf is modeled by a sum of Gaussians:\n",
    "$$p(x) = \\sum_{k=1}^N \\alpha_k \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "where $\\alpha_k$ is the \"mixing coefficient\" with $0\\le \\alpha_k \\le 1$ and $\\sum_{k=1}^N \\alpha_k = 1$.\n",
    "\n",
    "We can solve for the parameters using maximum likelihood analyis as we have discussed previously.\n",
    "However, this can be complicated in multiple dimensions, requiring the use of [**Expectation Maximization (EM)**](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expectation Maximization (ultra simplified version)\n",
    "\n",
    "(Note: all explanations of EM are far more complicated than seems necessary for our purposes, so here is my overly simplified explanation.)\n",
    "\n",
    "This may make more sense in terms of our earlier Bayesian analyses if we write this as \n",
    "$$p(z=c) = \\alpha_k,$$\n",
    "and\n",
    "$$p(x|z=c) = \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "where $z$ is a \"hidden\" variable related to which \"component\" each point is assigned to.\n",
    "\n",
    "In the Expectation step, we hold $\\mu_k, \\Sigma_k$, and $\\alpha_k$ fixed and compute the probability that each $x_i$ belongs to component, $c$.  \n",
    "\n",
    "In the Maximization step, we hold the probability of the components fixed and maximize $\\mu_k, \\Sigma_k,$ and $\\alpha_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that $\\alpha$ is the relative weight of each Gaussian component and not the probability of each point belonging to a specific component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use the following animation to illustrate the process.  \n",
    "\n",
    "We start with a 2-component GMM, where the initial components can be randomly determined.\n",
    "\n",
    "The points that are closest to the centroid of a component will be more probable under that distribution in the \"E\" step and will pull the centroid towards them in the \"M\" step.  Iteration between the \"E\" and \"M\" step eventually leads to convergence.\n",
    "\n",
    "In this particular example, 3 components better describes the data and similarly converges.  Note that the process is not that sensitive to how the components are first initialized.  We pretty much get the same result in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"B36fzChfyGU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A typical call to the [Gaussian Mixture Model](http://scikit-learn.org/stable/modules/mixture.html) algorithm looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.mixture import GMM\n",
    "\n",
    "X = np.random.normal(size=(1000,2)) #1000  points in 2D\n",
    "gmm = GMM(3) #three components\n",
    "gmm.fit(X)\n",
    "log_dens = gmm.score(X)\n",
    "BIC = gmm.bic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with the 1-D example given in Ivezic, Figure 6.8, which compares a Mixture Model to KDE.\n",
    "[Note that the version at astroML.org has some bugs!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.8\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from astroML.plotting import hist\n",
    "from sklearn.mixture import GMM\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate our data: a mix of several Cauchy distributions\n",
    "#  this is the same data used in the Bayesian Blocks figure\n",
    "np.random.seed(0)\n",
    "N = 10000\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "true_pdf = lambda x: sum([f * stats.cauchy(mu, gamma).pdf(x)\n",
    "                          for (mu, gamma, f) in mu_gamma_f])\n",
    "x = np.concatenate([stats.cauchy(mu, gamma).rvs(int(f * N))\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "np.random.shuffle(x)\n",
    "x = x[x > -10]\n",
    "x = x[x < 30]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.subplots_adjust(bottom=0.08, top=0.95, right=0.95, hspace=0.1)\n",
    "N_values = (500, 5000)\n",
    "subplots = (211, 212)\n",
    "k_values = (10, 100)\n",
    "\n",
    "for N, k, subplot in zip(N_values, k_values, subplots):\n",
    "    ax = fig.add_subplot(subplot)\n",
    "    xN = x[:N]\n",
    "    t = np.linspace(-10, 30, 1000)\n",
    "\n",
    "    kde = KernelDensity(0.1, kernel='gaussian')\n",
    "    kde.fit(xN[:, None])\n",
    "    dens_kde = np.exp(kde.score_samples(t[:, None]))\n",
    "\n",
    "    # Compute density via Gaussian Mixtures\n",
    "    # we'll try several numbers of clusters\n",
    "    n_components = np.arange(3, 16)\n",
    "    gmms = [GMM(n_components=n).fit(xN[:,None]) for n in n_components]\n",
    "    BICs = [gmm.bic(xN[:,None]) for gmm in gmms]\n",
    "    i_min = np.argmin(BICs)\n",
    "    t = np.linspace(-10, 30, 1000)\n",
    "    logprob, responsibilities = gmms[i_min].score_samples(t[:,None])\n",
    "\n",
    "    # plot the results\n",
    "    ax.plot(t, true_pdf(t), ':', color='black', zorder=3,\n",
    "            label=\"Generating Distribution\")\n",
    "    ax.plot(xN, -0.005 * np.ones(len(xN)), '|k', lw=1.5)\n",
    "    ax.plot(t, np.exp(logprob), '-', color='gray',\n",
    "            label=\"Mixture Model\\n(%i components)\" % n_components[i_min])\n",
    "    ax.plot(t, dens_kde, '-', color='black', zorder=3,\n",
    "            label=\"Kernel Density $(h=0.1)$\")\n",
    "\n",
    "    # label the plot\n",
    "    ax.text(0.02, 0.95, \"%i points\" % N, ha='left', va='top',\n",
    "            transform=ax.transAxes)\n",
    "    ax.set_ylabel('$p(x)$')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    if subplot == 212:\n",
    "        ax.set_xlabel('$x$')\n",
    "\n",
    "    ax.set_xlim(0, 20)\n",
    "    ax.set_ylim(-0.01, 0.4001)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hmm, that doesn't look so great for the 5000 point distribution.  Plot the BIC values and see if anything looks awry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do the individual components look like?  Make a plot of those.  Careful with the shapes of the arrays!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can you figure out something that you can do to improve the results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ivezic, Figure 6.6 shows a 2-D example.  In the first panel, we have the raw data.  In the second panel we have a density plot (essentially a 2-D histogram).  We then try to represent the data with a series of Gaussians.  We allow up to 14 Gaussians and use the AIC/BIC to determine the best choice for this number.  This is shown in the third panel.  Finally, the fourth panel shows the chosen Gaussians with their centroids and 1-$\\sigma$ contours.\n",
    "\n",
    "In this case 7 components are required for the best fit.  While it looks like we could do a pretty good job with just 2 components, there does appear to be some \"background\" that is a high enough level to justify further components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.6\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.mixture import GMM\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "from astroML.decorators import pickle_results\n",
    "from astroML.plotting.tools import draw_ellipse\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get the Segue Stellar Parameters Pipeline data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "\n",
    "# Note how X was created from two columns of data\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute GMM models & AIC/BIC\n",
    "N = np.arange(1, 14)\n",
    "\n",
    "#@pickle_results(\"GMM_metallicity.pkl\")\n",
    "def compute_GMM(N, covariance_type='full', n_iter=1000):\n",
    "    models = [None for n in N]\n",
    "    for i in range(len(N)):\n",
    "        #print N[i]\n",
    "        models[i] = GMM(n_components=N[i], n_iter=n_iter, covariance_type=covariance_type)\n",
    "        models[i].fit(X)\n",
    "    return models\n",
    "\n",
    "models = compute_GMM(N)\n",
    "\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "i_best = np.argmin(BIC)\n",
    "gmm_best = models[i_best]\n",
    "print \"best fit converged:\", gmm_best.converged_\n",
    "print \"BIC: n_components =  %i\" % N[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute 2D density\n",
    "FeH_bins = 51\n",
    "alphFe_bins = 51\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'], (FeH_bins, alphFe_bins))\n",
    "\n",
    "Xgrid = np.array(map(np.ravel,\n",
    "                     np.meshgrid(0.5 * (FeH_bins[:-1]\n",
    "                                        + FeH_bins[1:]),\n",
    "                                 0.5 * (alphFe_bins[:-1]\n",
    "                                        + alphFe_bins[1:])))).T\n",
    "log_dens = gmm_best.score(Xgrid).reshape((51, 51))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.subplots_adjust(wspace=0.45, bottom=0.25, top=0.9, left=0.1, right=0.97)\n",
    "\n",
    "# plot data\n",
    "ax = fig.add_subplot(141)\n",
    "ax.scatter(data['FeH'][::10],data['alphFe'][::10],marker=\".\",color='k',edgecolors='None')\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.text(0.93, 0.93, \"Input\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "# plot density\n",
    "ax = fig.add_subplot(142)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.text(0.93, 0.93, \"Density\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "# plot AIC/BIC\n",
    "ax = fig.add_subplot(143)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, ':k', label='BIC')\n",
    "ax.legend(loc=1)\n",
    "ax.set_xlabel('N components')\n",
    "plt.setp(ax.get_yticklabels(), fontsize=7)\n",
    "\n",
    "# plot best configurations for AIC and BIC\n",
    "ax = fig.add_subplot(144)\n",
    "ax.imshow(np.exp(log_dens),\n",
    "          origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "ax.scatter(gmm_best.means_[:, 0], gmm_best.means_[:, 1], c='w')\n",
    "for mu, C, w in zip(gmm_best.means_, gmm_best.covars_, gmm_best.weights_):\n",
    "    draw_ellipse(mu, C, scales=[1], ax=ax, fc='none', ec='k')\n",
    "\n",
    "ax.text(0.93, 0.93, \"Converged\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That said, I'd say that there are *too* many components here.  So, I'd be inclined to explore this a bit further if it were my data.\n",
    "\n",
    "Lastly, let's look at a 2-D case where we are using GMM more to characterize the data than to find clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.7\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.mixture import GMM\n",
    "from astroML.datasets import fetch_great_wall\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# load great wall data\n",
    "X = fetch_great_wall()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create a function which will save the results to a pickle file\n",
    "#  for large number of clusters, computation will take a long time!\n",
    "#@pickle_results('great_wall_GMM.pkl')\n",
    "def compute_GMM(n_clusters, n_iter=1000, min_covar=3, covariance_type='full'):\n",
    "    clf = GMM(n_clusters, covariance_type=covariance_type,\n",
    "              n_iter=n_iter, min_covar=min_covar)\n",
    "    clf.fit(X)\n",
    "    print \"converged:\", clf.converged_\n",
    "    return clf\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a grid on which to evaluate the result\n",
    "Nx = 100\n",
    "Ny = 250\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx),\n",
    "                                            np.linspace(ymin, ymax, Ny)))).T\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#\n",
    "# we'll use 100 clusters.  In practice, one should cross-validate\n",
    "# with AIC and BIC to settle on the correct number of clusters.\n",
    "clf = compute_GMM(n_clusters=100)\n",
    "log_dens = clf.score(Xgrid).reshape(Ny, Nx)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(hspace=0, left=0.08, right=0.95, bottom=0.13, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(211, aspect='equal')\n",
    "ax.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "\n",
    "ax.set_xlim(ymin, ymax)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "plt.ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "ax = fig.add_subplot(212, aspect='equal')\n",
    "ax.imshow(np.exp(log_dens.T), origin='lower', cmap=plt.cm.binary,\n",
    "          extent=[ymin, ymax, xmin, xmax])\n",
    "ax.set_xlabel(r'$y\\ {\\rm (Mpc)}$')\n",
    "ax.set_ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that this is very different than the non-parametric density estimates that we did last time in that the GMM isn't doing that great of a job of matching the distribution.  However, the advantage is that we now have a *model*.  This model can be stored very compactly with just a few numbers, unlike the KDE or KNN maps which require a floating point number for each grid point.  \n",
    "\n",
    "One thing that you might imagine doing with this is subtracting the model from the data and looking for interesting things among the residuals."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
