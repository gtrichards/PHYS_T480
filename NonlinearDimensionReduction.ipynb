{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear Dimensionality Reduction\n",
    "G. Richards (2016), based on materials from Ivezic, Connolly, Miller, Leighly, and VanderPlas.\n",
    "\n",
    "Today we will talk about the concepts of \n",
    "* manifold learning\n",
    "* nonlinear dimensionality reduction\n",
    "\n",
    "Specifically using the following algorithms\n",
    "* local linear embedding (LLE)\n",
    "* isometric mapping (IsoMap)\n",
    "* t-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start by my echoing the brief note of caution given in Adam Miller's notebook: \"astronomers will often try to derive physical insight from PCA eigenspectra or eigentimeseries, but this is not advisable as there is no physical reason for the data to be linearly and orthogonally separable\".  Moreover, physical components are (generally) positive definite.  So, PCA is great for dimensional reduction, but for doing physics there are generally better choices.\n",
    "\n",
    "While NMF \"solves\" the issue of negative components, it is still a linear process.  For data with non-linear correlations, an entire field, known as [Manifold Learning](http://scikit-learn.org/stable/modules/manifold.html) and [nonlinear dimensionality reduction]( https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction), has been developed, with several algorithms available via the [`sklearn.manifold`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold) module. \n",
    "\n",
    "For example, if your data set looks like this:\n",
    "![Ivezic, Figure 7.8a](images/IvezicFigure7p8a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then PCA is going to give you something like this.  \n",
    "\n",
    "![Ivezic, Figure 7.8b](images/IvezicFigure7p8b.png)\n",
    "\n",
    "Clearly not very helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What you really want is something more like the results below.  For more examples see\n",
    "[Vanderplas & Connolly 2009](http://iopscience.iop.org/article/10.1088/0004-6256/138/5/1365/meta;jsessionid=48A569862A424ECCAEECE2A900D9837B.c3.iopscience.cld.iop.org)\n",
    "\n",
    "![Ivezic, Figure 7.8cd](images/IvezicFigure7p8cd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local Linear Embedding\n",
    "\n",
    "[Local Linear Embedding](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) attempts to embed high-$D$ data in a lower-$D$ space.  Crucially it also seeks to preserve the geometry of the local \"neighborhoods\" around each point.  In the case of the \"S\" curve, it seeks to unroll the data.  The steps are\n",
    "\n",
    "Step 1: define local geometry\n",
    "- local neighborhoods determined from $k$ nearest neighbors.\n",
    "- for each point calculate weights that reconstruct a point from its $k$ nearest\n",
    "neighbors via\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\mathcal{E}_1(W) = \\left|X - WX\\right|^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "where $X$ is an $N\\times K$ matrix and $W$ is an $N\\times N$ matrix that minimizes the reconstruction error.\n",
    "\n",
    "Essentially this is finding the hyperplane that describes the local surface at each point within the data set. So, imagine that you have a bunch of square tiles and you are trying to tile the surface with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 2: embed within a lower dimensional space\n",
    "- set all $W_{ij}=0$ except when point $j$ is one of the $k$ nearest neighbors of point $i$.  \n",
    "- $W$ becomes very sparse for $k \\ll N$ (only $Nk$ entries in $W$ are non-zero). \n",
    "- minimize\n",
    ">$\\begin{equation}\n",
    "  \\mathcal{E}_2(Y) = \\left|Y - W Y\\right|^2,\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "with $W$ fixed to find an $N$ by $d$ matrix ($d$ is the new dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 1 requires a nearest-neighbor search.\n",
    "\n",
    "Step 2 requires an\n",
    "eigenvalue decomposition of the matrix $C_W \\equiv (I-W)^T(I-W)$.\n",
    "\n",
    "\n",
    "LLE has been applied to data as diverse as galaxy spectra, stellar spectra, and photometric light curves.   It was introduced by [Roweis & Saul (2000)](https://www.ncbi.nlm.nih.gov/pubmed/11125150).\n",
    "\n",
    "Skikit-Learn's call to LLE is as follows, with a more detailed example already being given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "X = np.random.normal(size=(1000,2)) # 1000 points in 2D\n",
    "R = np.random.random((2,10)) # projection matrix\n",
    "X = np.dot(X,R) # now a 2D linear manifold in 10D space\n",
    "k = 5 # Number of neighbors to use in fit\n",
    "n = 2 # Number of dimensions to fit\n",
    "lle = LocallyLinearEmbedding(k,n)\n",
    "lle.fit(X)\n",
    "proj = lle.transform(X) # 100x2 projection of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "See what LLE does for the digits data, using the 7 nearest neighbors and 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell to load the digits sample\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from matplotlib import pyplot as plt\n",
    "digits = load_digits()\n",
    "grid_data = np.reshape(digits.data[0], (8,8)) #reshape to 8x8\n",
    "plt.imshow(grid_data, interpolation = \"nearest\", cmap = \"bone_r\")\n",
    "print grid_data\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#LLE\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "# Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Isometric Mapping\n",
    "\n",
    "is based on multi-dimensional scaling (MDS) framework.  It was introduced in the same volume of science as the article above.  See [Tenenbaum, de Silva, & Langford (2000)](https://www.ncbi.nlm.nih.gov/pubmed/?term=A+Global+Geometric+Framework+for+Nonlinear+Dimensionality+Reduction).\n",
    "Geodestic curves are used to recover non-linear structure.\n",
    "\n",
    "In Scikit-Learn [IsoMap](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html) is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.manifold import Isomap\n",
    "XX = np.random.normal(size=(1000,2)) # 1000 points in 2D\n",
    "R = np.random.random((2,10)) # projection matrix\n",
    "XX = np.dot(XX,R) # X is a 2D manifold in 10D space\n",
    "k = 5 # number of neighbors\n",
    "n = 2 # number of dimensions\n",
    "iso = Isomap(k,n)\n",
    "iso.fit(XX)\n",
    "proj = iso.transform(XX) # 1000x2 projection of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try 7 neighbors and 2 dimensions on the digits data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# IsoMap\n",
    "from sklearn.manifold import Isomap\n",
    "# Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## t-SNE\n",
    "\n",
    "[t-distributed Stochastic Neighbor Embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is not discussed in the book, Scikit-Learn does have a [t-SNE implementation](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) and it is well worth mentioning this manifold learning algorithm too.  SNE itself was developed by [Hinton & Roweis](http://www.cs.toronto.edu/~fritz/absps/sne.pdf) with the \"$t$\" part being added by [van der Maaten & Hinton](http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf).  It works like the other manifold learning algorithms.  Try it on the digits data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "# Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You'll know if you have done it right if you understand Adam Miller's comment \"Holy freakin' smokes.  That is magic.  (It's possible we just solved science).\"\n",
    "\n",
    "Personally, I think that some exclamation points may be needed in there!\n",
    "\n",
    "What's even more illuminating is to make the plot using the actual digits to plot the points.  Then you can see why certain digits are alike or split into multiple regions.  Can you explain the patterns you see here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Scale and visualize the embedding vectors\n",
    "def plot_embedding(X):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "      #plt.text(X[i, 0], X[i, 1], str(digits.target[i]), color=plt.cm.Set1(y[i] / 10.), fontdict={'weight': 'bold', 'size': 9})\n",
    "      plt.text(X[i, 0], X[i, 1], str(digits.target[i]), color=plt.cm.nipy_spectral(y[i]/9.))\n",
    "\n",
    "\n",
    "    shown_images = np.array([[1., 1.]])  # just something big\n",
    "    for i in range(digits.data.shape[0]):\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 4e-3:\n",
    "            # don't show points that are too close\n",
    "            continue\n",
    "        shown_images = np.r_[shown_images, [X[i]]]\n",
    "        imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i])\n",
    "        ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    \n",
    "plot_embedding(X_reduced)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With the remainder of time in class today, play with the arguments of the algorithms that we have discussed this week and/or try running them on a different data set.  For example the iris data set or one of the other samples of data that are included with Scikit-Learn.  Or maybe have a look through some of these public data repositories:\n",
    "\n",
    "- [https://github.com/caesar0301/awesome-public-datasets?utm_content=buffer4245d&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer](https://github.com/caesar0301/awesome-public-datasets?utm_content=buffer4245d&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)\n",
    "- [http://www.datasciencecentral.com/m/blogpost?id=6448529%3ABlogPost%3A318739](http://www.datasciencecentral.com/m/blogpost?id=6448529%3ABlogPost%3A318739)\n",
    "- [http://www.kdnuggets.com/2015/04/awesome-public-datasets-github.html](http://www.kdnuggets.com/2015/04/awesome-public-datasets-github.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
