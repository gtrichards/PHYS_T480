{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification\n",
    "\n",
    "G. Richards (2016), based particularly on materials from Andy Connolly, also Ivezic.\n",
    "\n",
    "Density estimation and clustering are **unsupervised** forms of classification.  Let's now move on to **supervised**\n",
    "classification.  That's where we actually know the \"truth\" for some of our objects and can use that information to help guide the classification of unknown objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative vs. Discriminative Classification\n",
    "\n",
    "We will talk about two different types of classification where each has a slightly different approach.  As an example, if you are trying to determine whether your neighbor is speaking Spanish or Portuguese, you could 1) learn both Spanish and Portuguese so that you'd know exactly what they are saying or 2) learn the keys rules about the differences between the languages.\n",
    "\n",
    "If we find ourselves asking which category is most likely to generate the observed result, then we are using using **density estimation** for classification and this is referred to as **generative classification**.  Here we have a full model of the density for each class or we have a model which describes how data could be generated from each class. \n",
    "\n",
    "If, on the other hand, we don't care about the full distribution, then we are doing something more like clustering, where we don't need to map the distribution, we just need to define boundaries.  Classification that finds the **decision boundary** that separates classes is called **discriminative classification**.  For high-dimensional data, this may be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, in the figure below, to classify a new object with $x=1$, it would suffice to know that either 1) model 1 is a better fit than model 2, or 2) that the decision boundary is at $x=1.4$.\n",
    "\n",
    "![Ivezic, Figure 9.1](http://www.astroml.org/_images/fig_bayes_DB_1.png)\n",
    "\n",
    "In my work, we actually do both.  We first do discriminative classification using a decision boundary based on $KD$ trees and then we do generative classification using density estimation for the class of interest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scoring Your Results\n",
    "\n",
    "The first question that we need to address is how we score our results (defined the success of our classification).\n",
    "\n",
    "In the simplest case, there are 2 types of errors:\n",
    "* a [False Positive](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_positive_error), where we have assigned a *true* class label when it is really false.  This is called a \"Type-1 error\".\n",
    "* a [False Negative](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_positive_error), where we have assigned a *false* class label when it is really true.  This is called a \"Type-II error\".\n",
    "\n",
    "All 4 [possibilities](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) are:\n",
    "- True Positive = correctly identified\n",
    "- False Negative = incorrectly rejected\n",
    "- True Negative = correctly rejected\n",
    "- False Positive = incorrectly identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Based on these, we usually define either of these pairs of terms.  Which is used is largely a matter of preference in different fields, but we'll see that there are some key differences.\n",
    "\n",
    ">$ {\\rm completeness} = \\frac{\\rm true\\ positives}{\\rm true\\ positives + false\\ negatives}$\n",
    "\n",
    ">$  {\\rm contamination} = \\frac{\\rm false\\ positives}{\\rm true\\ positives + false\\ positives} = {\\rm false\\ discovery\\ rate}$\n",
    "\n",
    "or\n",
    "\n",
    "> $ {\\rm true\\ positive\\ rate} = \\frac{\\rm true\\ positives} {\\rm true\\ positives + false\\ negatives}\n",
    "$\n",
    "\n",
    "> $  {\\rm false\\ positive\\ rate} = \\frac{\\rm false\\ positives} {\\rm true\\ negatives + false\\ positives} = {\\rm Type1\\ error}\n",
    "$\n",
    "\n",
    "where **completeness** = **true positive rate** and this is also called **sensitivity** or **recall**.\n",
    "\n",
    "Similarly \n",
    " \n",
    ">$ {\\rm efficiency} = 1 - {\\rm contamination} = {\\rm precision}. $\n",
    "\n",
    "Scikit-Learn also reports the **F1 score** which is the harmonic mean of precision and sensitivity (efficiency and completeness).\n",
    "\n",
    "Depending on your goals, you may want to maximize the completeness or the efficiency, or a combination of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example you might want to minimize voter fraud (contamination), but if doing so reduced voter participation (completeness) by a larger amount, then that wouldn't be such a good thing.  So you need to decide what balance it is that you want to strike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To better understand the differences between these measures, let's take the needle in a haystack problem.  You have 100,000 stars and 1000 quasars.  If you correctly identify 900 quasars as such and mistake 1000 stars for quasars, then we have:\n",
    "- TP = 900\n",
    "- FN  = 100\n",
    "- TN = 99,000\n",
    "- FP = 1000\n",
    "\n",
    "Which gives\n",
    "\n",
    "> $ {\\rm true\\ positive\\ rate} = \\frac{900}{900 + 100} = 0.9 = {\\rm completeness}\n",
    "$\n",
    "\n",
    "> $  {\\rm false\\ positive\\ rate} = \\frac{1000}{99000 + 1000} = 0.01\n",
    "$\n",
    "\n",
    "Not bad right?  Well, sort of.  The FPR isn't bad, but there are *lots* of stars, so the contamination rate isn't so great:\n",
    "\n",
    "> $  {\\rm contamination} = \\frac{1000}{900 + 1000} = 0.53\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing the performance of classifiers\n",
    "\n",
    "So, \"best\" performance is a bit of a subjective topic. We trade contamination as a function of completeness and this is science dependent.\n",
    "\n",
    "Before we start talking about different classification algorithms, let's first talk about how we can quantify which of the methods is \"best\".  (N.B.  We have skipped ahead to Ivezic $\\S$ 9.8).  \n",
    "\n",
    "The way that we will do this is with a [**Receiver Operating Characteristic (ROC)**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve.  (Apparently this is *yet another* example in statistics/machine learning where the name of something was deliberately chosen to scare people away from the field.)   A ROC curve simply plots the true-positive vs. the false-positive rate.  \n",
    "\n",
    "One concern about ROC curves is that they are sensitive to the relative sample sizes.  As we already demonstrated above, if there are many more background events than source events small false positive results can dominate a signal. For these cases we can plot efficiency (1 - contamination) vs. completeness.\n",
    "\n",
    "Indeed, I had never even heard of a ROC curve until I started preparing this class.  I have always made \"completeness-contamination\" plots, which makes a lot more sense to me (both in terms of what can be learned and nomenclature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a comparison of the two types of plots:\n",
    "\n",
    "![Ivezic, Figure 9.17](http://www.astroml.org/_images/fig_ROC_curve_1.png)\n",
    "\n",
    "Here we see that to get higher completeness, you could actually suffer significantly in terms of efficiency, but your FPR might not go up that much if there are lots of true negatives.  I'll point this out again later when we do a specific example.\n",
    "\n",
    "Generally speaking, you want to chose a decision boundary (see below) that maximizes the area under the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below is the code that makes these plots.  We'll talk about the data that goes into it in a bit.  For now, we'll concentrate on how to generate the ROC and completeness-contamination plots.\n",
    "\n",
    "We'll be comparing 7 different classifiers (with a generic `clf` object), making training and test sets with `split_samples`, then using these tools to generate our plots:\n",
    "\n",
    "- [sklearn.metrics.roc_curve(y_test, y_prob)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    "- [sklearn.metrics.precision_recall_curve(y_test, y_prob)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)\n",
    "- astroML.utils.completeness_contamination(y_pred, y_test)\n",
    "\n",
    "Note that the [`sklearn.metrics` algorithms](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) take `y_test`, which are classes, and `y_prob`, which are not class predictions, but rather probabilities, whereas the AstroML algorithm wants `y_pred` (which we get by converting `y_prob` into discrete predictions as a function of the probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Ivezic, Figure 9.17\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.lda import LDA\n",
    "#from sklearn.qda import QDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from astroML.classification import GMMBayes\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "from astroML.utils import split_samples, completeness_contamination\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "y = y.astype(int)\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fit all the models to the training data\n",
    "def compute_models(*args):\n",
    "    names = []\n",
    "    probs = []\n",
    "    for classifier, kwargs in args:\n",
    "        clf = classifier(**kwargs)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        names.append(classifier.__name__)\n",
    "        probs.append(y_probs)\n",
    "\n",
    "    return names, probs\n",
    "\n",
    "names, probs = compute_models((GaussianNB, {}),\n",
    "                              (LinearDiscriminantAnalysis, {}),\n",
    "                              (QuadraticDiscriminantAnalysis, {}),\n",
    "                              (LogisticRegression,\n",
    "                               dict(class_weight='auto')),\n",
    "                              (KNeighborsClassifier,\n",
    "                               dict(n_neighbors=10)),\n",
    "                              (DecisionTreeClassifier,\n",
    "                               dict(random_state=0, max_depth=12,\n",
    "                                    criterion='entropy')),\n",
    "                              (GMMBayes, dict(n_components=3, min_covar=1E-5,\n",
    "                                              covariance_type='full')))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot ROC curves and completeness/efficiency\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.15, top=0.9, wspace=0.25)\n",
    "\n",
    "# ax1 will show roc curves\n",
    "ax1 = plt.subplot(131)\n",
    "\n",
    "# ax2 will show precision/recall\n",
    "ax2 = plt.subplot(132)\n",
    "\n",
    "# ax3 will show completeness/efficiency\n",
    "ax3 = plt.subplot(133)\n",
    "\n",
    "labels = dict(GaussianNB='GNB',\n",
    "              LinearDiscriminantAnalysis='LDA',\n",
    "              QuadraticDiscriminantAnalysis='QDA',\n",
    "              KNeighborsClassifier='KNN',\n",
    "              DecisionTreeClassifier='DT',\n",
    "              GMMBayes='GMMB',\n",
    "              LogisticRegression='LR')\n",
    "\n",
    "thresholds = np.linspace(0, 1, 1001)[:-1]\n",
    "\n",
    "# iterate through and show results\n",
    "for name, y_prob in zip(names, probs):\n",
    "    # Note that these take y_prob and not y_pred\n",
    "    fpr, tpr, thresh = roc_curve(y_test, y_prob)\n",
    "    precision, recall, thresh2 = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "    # add (0, 0) as first point\n",
    "    fpr = np.concatenate([[0], fpr])\n",
    "    tpr = np.concatenate([[0], tpr])\n",
    "    precision = np.concatenate([[0], precision])\n",
    "    recall = np.concatenate([[1], recall])\n",
    "\n",
    "    ax1.plot(fpr, tpr, label=labels[name])\n",
    "    ax2.plot(precision, recall, label=labels[name])\n",
    "    \n",
    "    # Whereas this does take y_pred, which we need to compute\n",
    "    # by looping through all possible probability thresholds\n",
    "    comp = np.zeros_like(thresholds)\n",
    "    cont = np.zeros_like(thresholds)\n",
    "    for i, t in enumerate(thresholds):\n",
    "        y_pred = (y_prob >= t)\n",
    "        comp[i], cont[i] = completeness_contamination(y_pred, y_test)\n",
    "    ax3.plot(1-cont, comp, label=labels[name])\n",
    "\n",
    "ax1.set_xlim(0, 0.04)\n",
    "ax1.set_ylim(0, 1.02)\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "ax1.set_xlabel('false positive rate')\n",
    "ax1.set_ylabel('true positive rate')\n",
    "ax1.legend(loc=4)\n",
    "\n",
    "ax2.set_xlabel('precision')\n",
    "ax2.set_ylabel('recall')\n",
    "ax2.set_xlim(0, 1.0)\n",
    "ax2.set_ylim(0.2, 1.02)\n",
    "\n",
    "ax3.set_xlabel('efficiency')\n",
    "ax3.set_ylabel('completeness')\n",
    "ax3.set_xlim(0, 1.0)\n",
    "ax3.set_ylim(0.2, 1.02)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that I've plotted both recall-precision and completeness-efficiency just to demonstrate that they are the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Classification\n",
    "\n",
    "We can use Bayes' theorem to relate the labels to the features in an $N\\times D$ data set $X$.  The $j$th feature of the $i$th point is $x_{ij}$ and there are $k$ classes giving discrete labels $y_k$.  Then we have\n",
    "\n",
    "$$p(y_k|x_i) = \\frac{p(x_i|y_k)p(y_k)}{\\sum_i p(x_i|y_k)p(y_k)},$$\n",
    "\n",
    "where $x_i$ is assumed to be a vector with $j$ components.\n",
    "\n",
    "$p(y=y_k)$ is the probability of any point having class $k$ (equivalent to the prior probability of the class $k$). \n",
    "\n",
    "In generative classifiers we model class-conditional densities $p_k(x) = p(x|y=y_k)$ and our goal is to estimate the $p_k$'s. \n",
    "\n",
    "Before we get into the generative classification algortithms, we'll first discuss 3 general concepts:\n",
    "- Discriminant Functions\n",
    "- Bayes Classifiers\n",
    "- Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Discriminant Function\n",
    "\n",
    "We can relate classification to density estimation and regression.\n",
    "\n",
    "$\\hat{y} = f(y|x)$ represents the best guess of $y$ given $x$.  So classification can be thought of as the analog of regression where $y$ is a discrete *category* rather than a continuous variable, for example $y=\\{0,1\\}$.\n",
    "\n",
    "\n",
    "In classification we refer to $f(y|x)$ as the [**discriminant function**](https://en.wikipedia.org/wiki/Discriminant_function_analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a simple 2-class example:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "g(x) = f(y|x) & = &  \\int y \\, p(y|x) \\, dy \\\\\n",
    "%    & = & \\int y p(y|x) \\, dy \\\\\n",
    "       & = & 1 \\cdot p(y=1 | x) + 0 \\cdot p(y=0 | x) = p(y=1 | x).\n",
    "%     & = & p(y=1 | x)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "From Bayes rule:\n",
    "\n",
    "$$g(x) = \\frac{p(x|y=1) \\, p(y=1)}{p(x|y=1) \\, p(y=1)  + p(x|y=0) \\, p(y=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Classifier\n",
    "\n",
    "If the discriminant function gives a binary prediction, we call it a **Bayes classifier**, formulated as\n",
    "\n",
    "$$\\begin{eqnarray} \\widehat{y} & = & \\left\\{ \\begin{array}{cl}       \t           1 & \\mbox{if $g(x) > 1/2$}, \\\\       \t           0 & \\mbox{otherwise,}       \t           \\end{array}     \t   \\right. \\\\     & = & \\left\\{\n",
    "\\begin{array}{cl}               1 & \\mbox{if $p(y=1|x) > p(y=0|x)$}, \\\\               0 & \\mbox{otherwise.}               \\end{array}       \\right.\\end{eqnarray}$$\n",
    "\n",
    "Where this can be generalized to any number of classes, $k$, and not just two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Boundary\n",
    "\n",
    "A **decision boundary** is just set of $x$ values at which each class is equally likely:\n",
    "\n",
    "$$\n",
    "p(x|y=1)p(y=1)  =  p(x|y=0)p(y=0);\n",
    "$$\n",
    "\n",
    "$$g_1(x) = g_2(x) \\; {\\rm or}\\; g(x) = 1/2$$\n",
    "\n",
    "Below is an example of a decision boundary in 1-D.  In short, we assign classifications according to which pdf is higher at every given $x$.\n",
    "\n",
    "![Ivezic, Figure 9.1](http://www.astroml.org/_images/fig_bayes_DB_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest Classifier: Naive Bayes\n",
    "\n",
    "In practice classification can be very complicated as the data are generally multi-dimensional (that is we don't just have $x$, we have $x_{j=0},x_1,x_2,x_3...x_n$, so we want $p(x_0,x_1,x_2,x_3...x_n|y)$.\n",
    "\n",
    "However, if we **assume** that all attributes are conditionally independent (which is not always true, but is often close enough), then this simplifies to\n",
    "\n",
    "$$ p(x_1,x_2|y_k) = p(x_1|y)p(x_2|y_k)$$\n",
    "  \n",
    "which can be written as\n",
    "\n",
    "$$ p({x_{j=0},x_1,x_2,\\ldots,x_N}|y_k) = \\prod_j p(x_j|y_k).$$\n",
    "\n",
    "From Bayes' rule and conditional independence we get\n",
    "\n",
    "$$\n",
    "  p(y_k | {x_0,x_1,\\ldots,x_N}) =\n",
    "  \\frac{\\prod_j p(x_j|y_k) p(y_k)}\n",
    "       {\\sum_l \\prod_j p(x_j|y_l) p(y_l)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We calculate the most likely value of $y$ by maximizing over $y_k$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{y_k} \\frac{\\prod_j p(x_j|y_k) p(y_k)}\n",
    "        {\\sum_l \\prod_j p(x_j|y_l) p(y_l)},\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From there the process is just estimating densities: $p(x|y=y_k)$ and $p(y=y_k)$ are learned from a set of training data, where\n",
    "- $p(y=y_k)$ is just the frequency of the class $k$ in the training set\n",
    "- $p(x|y=y_k)$ is just the density (probability) of an object with class $k$ having the attributes $x$\n",
    "\n",
    "A catch is that if the training set does not cover the full parameter space, then  $p(x_i|y=y_k)$ can be $0$ for some value of $y_k$ and $x_i$.  The posterior probability is then $p(y_k|\\{x_i\\}) = 0/0$ which is a problem! A trick called [**Laplace smoothing**](https://en.wikipedia.org/wiki/Laplacian_smoothing) can be implemented to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "It is totally unclear from the discussion in the book that $x_i$ are discrete measurements.  However, one way to handle continuous values for $X$ is to model $p(x_i|y=y_k)$ as one-dimensional normal distributions, with means $\\mu_{ik}$ and widths $\\sigma_{ik}$. The naive Bayes estimator is then\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{y_k}\\left[\\ln p(y=y_k) - \\frac{1}{2}\\sum_{i=1}^N\\left(2\\pi(\\sigma_{ik})^2 + \\frac{(x_i - \\mu_{ik})^2}{(\\sigma_{ik})^2} \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Scikit-Learn [`Gaussian Naive Bayes`](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) classification is implemented as follows, with a simple example given in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X,y)\n",
    "y_pred = gnb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Ivezic, Figure 9.2\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Simulate some data\n",
    "np.random.seed(0)\n",
    "mu1 = [1, 1]\n",
    "cov1 = 0.3 * np.eye(2)\n",
    "\n",
    "mu2 = [5, 3]\n",
    "cov2 = np.eye(2) * np.array([0.4, 0.1])\n",
    "\n",
    "X = np.concatenate([np.random.multivariate_normal(mu1, cov1, 100),\n",
    "                    np.random.multivariate_normal(mu2, cov2, 100)])\n",
    "y = np.zeros(200)\n",
    "y[100:] = 1\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fit the Naive Bayes classifier\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# predict the classification probabilities on a grid\n",
    "xlim = (-1, 8)\n",
    "ylim = (-1, 5)\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71), np.linspace(ylim[0], ylim[1], 81))\n",
    "xystack = np.vstack([xx.ravel(),yy.ravel()])\n",
    "Xgrid = xystack.T\n",
    "\n",
    "Z = clf.predict_proba(Xgrid)\n",
    "# Gives probability for both class 1 and class 2 for each grid point\n",
    "# As these are degenerate, take just one and then\n",
    "# re-shape it to the grid pattern needed for contour plotting\n",
    "Z = Z[:, 1].reshape(xx.shape)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the points\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.binary)\n",
    "\n",
    "# Add the decision boundary, which is just the contour where\n",
    "# the probability exceeds some threshold, here 0.5.\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And now an example using real data.  Here we have a 4-D $X$ and we are going to take them 1-D at a time to see how much improvement comes from adding each new dimension of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 9.3\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined() # X is a 4-D color-color-color-color space\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "\n",
    "N_tot = len(y)\n",
    "N_stars = np.sum(y == 0)\n",
    "N_rrlyrae = N_tot - N_stars\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rrlyrae\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# perform Naive Bayes\n",
    "\n",
    "# Create blank arrays to hold the output\n",
    "y_class = []\n",
    "y_pred = []\n",
    "y_probs = []\n",
    "\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "\n",
    "order = np.array([1, 0, 2, 3])\n",
    "\n",
    "for nc in Ncolors:\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train[:, :nc], y_train)\n",
    "    \n",
    "    y_pred.append(clf.predict(X_test[:, :nc]))\n",
    "    y_class.append(clf)    \n",
    "\n",
    "# Use astroML utils code to compute completeness and contamination\n",
    "completeness, contamination = completeness_contamination(y_pred, y_test)\n",
    "\n",
    "print \"completeness\", completeness\n",
    "print \"contamination\", contamination\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the decision boundary (for 2 colors)\n",
    "clf = y_class[1]\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 81),\n",
    "                     np.linspace(ylim[0], ylim[1], 71))\n",
    "\n",
    "Z = clf.predict_proba(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z[:, 1].reshape(xx.shape)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "im = ax.imshow(Z, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.binary, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 1.5)\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# Plot completeness vs Ncolors\n",
    "ax = plt.subplot(222)\n",
    "ax.plot(Ncolors, completeness, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# Plot contamination vs Ncolors\n",
    "ax = plt.subplot(224)\n",
    "ax.plot(Ncolors, contamination, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you shifted the decision boundary \"up\" by hand, what would happen to the completeness, contamination, and false positive rate?\n",
    "\n",
    "What happens if you change the fraction of objects in the training set?\n",
    "\n",
    "Extra Credit: Graph the false positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The \"naive\" refers to the fact that we are assuming that all of the variable are independent.  If we relax that assumption and allow for covariances, then we have a **Gaussian Bayes classifier**.  But note that this comes with a large jump in computational cost!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "In [Linear Discriminant Analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) we assume that the class distributions have identical\n",
    "covariances for all $k$ classes (all classes are a set of shifted Gaussians). \n",
    "\n",
    "<!---  The optimal classifier is derived from the log of the class posteriors --->\n",
    "\n",
    "<!--- > $g_k(\\vec{x}) = \\vec{x}^T \\Sigma^{-1} \\vec{\\mu_k} - \\frac{1}{2}\\vec{\\mu_k}^T \\Sigma^{-1} \\vec{\\mu_k} + \\log \\pi_k,$ --->\n",
    "\n",
    "<!--- with $\\vec{\\mu_k}$ the mean of class $k$ and $\\Sigma$ the covariance of the Gaussians. --->\n",
    "\n",
    "<!--- ** note different from book --->\n",
    "\n",
    "The class-dependent covariances that would normally give rise to a quadratic dependence on\n",
    "$X$ cancel out if they are assumed to be constant. The Bayes classifier is, therefore, linear with respect to $X$, and  discriminant boundary between classes is the line that minimizes\n",
    "the overlap between Gaussians.\n",
    "\n",
    "<!--- > $  g_k(\\vec{x}) - g_\\ell(\\vec{x}) = \\vec{x}^T \\Sigma^{-1} (\\mu_k-\\mu_\\ell)  - \\frac{1}{2}(\\mu_k - \\mu_\\ell)^T \\Sigma^{-1}(\\mu_k -\\mu_\\ell)  + \\log (\\frac{\\pi_k}{\\pi_\\ell}) = 0. $ --->\n",
    "\n",
    "Relaxing the requirement that the covariances of the\n",
    "Gaussians are constant, the discriminant function\n",
    "becomes quadratic in $X$.\n",
    "\n",
    "<!--- > $ g(\\vec{x}) = -\\frac{1}{2} \\log | \\Sigma_k |   - \\frac{1}{2}(\\vec{x}-\\mu_k)^T C^{-1}(\\vec{x}-\\mu_k) + \\log \\pi_k. $ --->\n",
    "\n",
    "This is sometimes known as [Quadratic Discriminant Analysis (QDA)](https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis).\n",
    "\n",
    "[`LDA`](http://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html) and [`QDA`](http://scikit-learn.org/0.16/modules/generated/sklearn.qda.QDA.html#sklearn.qda.QDA) are implemented in Scikit-Learn as follows and an example using the same data as above is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "lda = LDA()\n",
    "lda.fit(X,y)\n",
    "y_pred = lda.predict(X)\n",
    "\n",
    "qda = QDA()\n",
    "qda.fit(X,y)\n",
    "y_pred = qda.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figures 9.4 and 9.5\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "\n",
    "N_tot = len(y)\n",
    "N_stars = np.sum(y == 0)\n",
    "N_rrlyrae = N_tot - N_stars\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rrlyrae\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# perform LDA\n",
    "lda = LDA()\n",
    "lda.fit(X_train[:, :2], y_train)\n",
    "y_predLDA = lda.predict(X_test[:, :2])\n",
    "\n",
    "# perform QDA\n",
    "qda = QDA()\n",
    "qda.fit(X_train[:, :2], y_train)\n",
    "y_predQDA = qda.predict(X_test[:, :2])\n",
    "    \n",
    "completenessLDA, contaminationLDA = completeness_contamination(y_predLDA, y_test)\n",
    "completenessQDA, contaminationQDA = completeness_contamination(y_predQDA, y_test)\n",
    "\n",
    "print \"completeness\", completenessLDA, completenessQDA\n",
    "print \"contamination\", contaminationLDA, contaminationQDA\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the decision boundary\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71),\n",
    "                     np.linspace(ylim[0], ylim[1], 81))\n",
    "\n",
    "Z_LDA = lda.predict_proba(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z_LDA = Z_LDA[:, 1].reshape(xx.shape)\n",
    "Z_QDA = qda.predict_proba(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z_QDA = Z_QDA[:, 1].reshape(xx.shape)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "im = ax.imshow(Z_LDA, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.binary, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 1.5)\n",
    "\n",
    "ax.contour(xx, yy, Z_LDA, [0.5], linewidths=2., colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# right plot: qda\n",
    "ax = fig.add_subplot(122)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "im = ax.imshow(Z_QDA, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.binary, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 1.5)\n",
    "\n",
    "ax.contour(xx, yy, Z_QDA, [0.5], linewidths=2., colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If it is obvious from looking at your data that a linear or quadratic boundary will work well, then great.  But what if that is not the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GMM and Bayes Classification\n",
    "\n",
    "Our classifications so far have made some restrictive assumptions: either that of conditional independence or that the distributions are Gaussian.  However, a more flexible model might improve the completeness and efficiency of the classification.  For that we can look to the techniques from Chapter 6.\n",
    "\n",
    "The natural extension of the Gaussian assumptions is to use GMM's to determine the density distribution, i.e., a **GMM Bayes Classifier**.\n",
    "\n",
    "Note that the number of Gaussian components, $K$, must be chosen for each class, $k$, independently.\n",
    "\n",
    "astroML implements GMM Bayes classification as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astroML.classification import GMMBayes\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "gmmb = GMMBayes(3) # 3 clusters per class\n",
    "gmmb.fit(X,y)\n",
    "y_pred = gmmb.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now apply the GMM Bayes classifier to the real data from above.   With just one component, we get results that are similar to those from Naive Bayes.  But with 5 components (and all 4 attributes), we do pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 9.6\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.classification import GMMBayes\n",
    "#from astroML.decorators import pickle_results\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "\n",
    "# GMM-bayes takes several minutes to run, and is order[N^2]\n",
    "#  truncating the dataset can be useful for experimentation.\n",
    "#X = X[::10]\n",
    "#y = y[::10]\n",
    "\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "N_tot = len(y)\n",
    "N_stars = np.sum(y == 0)\n",
    "N_rrlyrae = N_tot - N_stars\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rrlyrae\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# perform GMM Bayes\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "Ncomp = [1, 5]\n",
    "\n",
    "def compute_GMMbayes(Ncolors, Ncomp):\n",
    "    classifiers = []\n",
    "    predictions = []\n",
    "\n",
    "    for ncm in Ncomp:\n",
    "        classifiers.append([])\n",
    "        predictions.append([])\n",
    "        for nc in Ncolors:\n",
    "            clf = GMMBayes(ncm, min_covar=1E-5, covariance_type='full')\n",
    "            clf.fit(X_train[:, :nc], y_train)\n",
    "            y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "            classifiers[-1].append(clf)\n",
    "            predictions[-1].append(y_pred)\n",
    "\n",
    "    return classifiers, predictions\n",
    "\n",
    "classifiers, predictions = compute_GMMbayes(Ncolors, Ncomp)\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print \"completeness\", completeness[0]\n",
    "print \"contamination\", contamination[0]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the decision boundary\n",
    "clf = classifiers[1][1]\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71),\n",
    "                     np.linspace(ylim[0], ylim[1], 81))\n",
    "\n",
    "Z = clf.predict_proba(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z[:, 1].reshape(xx.shape)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.binary, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "im = ax.imshow(Z, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.Oranges, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 1.5)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], linewidths=2., colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness[0], '^--k', c='k', label='N=%i' % Ncomp[0])\n",
    "ax.plot(Ncolors, completeness[1], 'o-k', c='k', label='N=%i' % Ncomp[1])\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination[0], '^--k', c='k', label='N=%i' % Ncomp[0])\n",
    "ax.plot(Ncolors, contamination[1], 'o-k', c='k', label='N=%i' % Ncomp[1])\n",
    "ax.legend(prop=dict(size=12),\n",
    "          loc='lower right',\n",
    "          bbox_to_anchor=(1.0, 0.78))\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can take this to the extreme by having one mixture component at each training point.  We also don't have to restrict ourselves to a Gaussian kernel, we can use any kernel that we like.  The resulting *non-parametric* Bayes classifier is referred to as **Kernel Discriminant Analysis (KDA)** .   It seems like this would be a *lot* more computationally intensive, but at least now we don't have to optimize the locations of the components, we just need to determine the bandwidth of the kernel.  In the end, it can result in better classification.\n",
    "\n",
    "One of the tricks to speed things up is that we don't need to know the actually class probability, we just need to know which is larger.  This is explained in more detail in [Riegel, Gray, & Richards 2008](http://epubs.siam.org/doi/abs/10.1137/1.9781611972788.19), and it is implemented in a series of papers starting with [Richards et al. 2004](http://adsabs.harvard.edu/abs/2004ApJS..155..257R).  \n",
    "\n",
    "If you follow those and you are so inclined, you could apply for jobs at [Skytree](http://www.skytree.net/) or [Wise.io](http://www.wise.io/) which were started by my collaborator, Alex Gray, and astronomer, Josh Bloom, respectively.\n",
    "\n",
    "It is worth noting that this illustrates one of the downsides of the book, astroML, and Scikit-learn.  They teach you about the basics of the algorithms, but if you wanted to use a **truly** big data set, then you really need the next level, such as KDA, but that is merely described here, not implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Nearest Neighbor Classifier\n",
    "\n",
    "If we did KDA with a variable bandwidth that depended only on the distance of the nearest neighbor, then we'd have what we call a **Nearest-Neighbor** classifier.  Here if $x$ is close to $x'$, then $p(y|x) \\approx p(y|x')$.  Note that we have not assumed anything about the conditional density distribution, so it is completely non-parametric.\n",
    "\n",
    "The number of neighbors, $K$, regulates the complexity of the classification, where a larger $K$ decreases the variance in the classification but leads to an increase in the bias.  (N.B., the 3rd different use of $K$ or $k$ in this notebook!)\n",
    "\n",
    "The distance measure is usually N-D Euclidean.  However, if the attributes have very different properties, then normalization, weighting, etc. may be needed.\n",
    "\n",
    "Scikit-learn implements [`K-Nearest Neighbors`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) classification as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "knc = KNeighborsClassifier(5) # use 5 nearest neighbors\n",
    "knc.fit(X,y)\n",
    "y_pred = knc.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Implementing it for the same example as above shows that it isn't all that great for this particular case.  See below.  We probably need more training data to reduce the variance for it to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "\n",
    "N_tot = len(y)\n",
    "N_st = np.sum(y == 0)\n",
    "N_rr = N_tot - N_st\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rr\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# perform Classification\n",
    "classifiers = []\n",
    "predictions = []\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "kvals = [1, 5]\n",
    "\n",
    "from sklearn import metrics\n",
    "    \n",
    "for k in kvals:\n",
    "    classifiers.append([])\n",
    "    predictions.append([])\n",
    "    for nc in Ncolors:\n",
    "        clf = KNeighborsClassifier(n_neighbors=k)\n",
    "        clf.fit(X_train[:, :nc], y_train)\n",
    "        y_pred = clf.predict(X_test[:, :nc])\n",
    "        \n",
    "        classifiers[-1].append(clf)\n",
    "        predictions[-1].append(y_pred)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print \"completeness (as a fn of neighbors and colors)\", completeness\n",
    "print \"contamination\", contamination\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the decision boundary\n",
    "clf = classifiers[1][1]\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71),\n",
    "                     np.linspace(ylim[0], ylim[1], 81))\n",
    "\n",
    "Z = clf.predict(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "im = ax.imshow(Z, origin='lower', aspect='auto',\n",
    "               cmap=plt.cm.binary, zorder=1,\n",
    "               extent=xlim + ylim)\n",
    "im.set_clim(0, 2)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], linewidths=2., colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "ax.text(0.02, 0.02, \"k = %i\" % kvals[1],\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "\n",
    "ax.plot(Ncolors, completeness[0], 'o-k', label='k=%i' % kvals[0])\n",
    "ax.plot(Ncolors, completeness[1], '^--k', label='k=%i' % kvals[1])\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination[0], 'o-k', label='k=%i' % kvals[0])\n",
    "ax.plot(Ncolors, contamination[1], '^--k', label='k=%i' % kvals[1])\n",
    "ax.legend(prop=dict(size=12),\n",
    "          loc='lower right',\n",
    "          bbox_to_anchor=(1.0, 0.79))\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Regardless of whether this is the best algorithm or not, we can choose $K$ to minimize the classifcation error rate by using cross-validation.  Give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "# New\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# perform Classification\n",
    "scores = []\n",
    "kvals = np.arange(1,20)\n",
    "for k in kvals:\n",
    "    clf = # Complete\n",
    "    CVpredk = cross_val_predict( # Complete\n",
    "    scores.append(accuracy_score( # Complete      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"max score is for k={:d}\".format(# Complete\n",
    "# Plot number of neighbors vs score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below is an example of another way to implement cross valideation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores2 = []\n",
    "for k in kvals:\n",
    "    # Let's do a 2-fold cross-validation of the SVC estimator\n",
    "    scores2.append(cross_val_score(KNeighborsClassifier(n_neighbors=k), X, y, cv=2, scoring='precision'))\n",
    "\n",
    "# Plot these too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also use the [`metrics` module](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) in sklearn to compute some statistics for us.  Try inserting the code below into the appropriate place in our nearest neighbors classifier above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "    \n",
    "        print k,nc\n",
    "        print(\"accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"precision:\", metrics.precision_score(y_test, y_pred))\n",
    "        print(\"recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
