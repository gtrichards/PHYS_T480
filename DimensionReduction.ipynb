{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensional Reduction\n",
    "G. Richards (2016), based on materials from Ivezic, Connolly, Leighly, and VanderPlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "You want to buy a car.  Right now--you don't want to wait.  But you are picky and have certain things that you would like it to have.  Each of those things has a probability between 0 and 1 of being on the the car dealer's lot.  You want a red car which has a probability of being on the lot of $p_{\\rm red}$; you want good gas mileage, $p_{\\rm gas}$; you want leather seats, $p_{\\rm leather}$; and you want a sunroof, $p_{\\rm sunroof}$.  The probability that the dealer has a car on the lot that meets all of those requirements is \n",
    "$$p_{\\rm red} \\, p_{\\rm gas} \\, p_{\\rm leather} \\, p_{\\rm sunroof},$$\n",
    "or $p^n$ where $n$ is the number of features (assuming equal probability for each).\n",
    "\n",
    "If the probability of each of these is 50%, then the probability of you driving off with your car of choice is only $0.5*0.5*0.5*0.5 = 0.0625$.  Not very good.  Imagine if you also wanted other things.  This is the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below we see an illustration of what happens when we split a distribution of random numbers in half (like for our car example) with each increasing dimension.\n",
    "\n",
    "![http://www.newsnshit.com/curse-of-dimensionality-interactive-demo/](http://i1.wp.com/www.newsnshit.com/wp-content/uploads/2014/10/580f1__Curse-of-dimensionality-620x549.png?resize=620%2C549)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mathematically we can describe this as: the more dimensions that your data span, the more points needed to uniformly sample the space.  \n",
    "\n",
    "For $D$ dimensions with coordinates $[-1,1]$, the fraction of points in a unit hypersphere (with radius $r$) is\n",
    "$$f_D = \\frac{V_D(r)}{(2r)^D} = \\frac{\\pi^{D/2}}{D2^{D-1}\\Gamma(D/2)}$$\n",
    "which goes to $0$ as $D$ goes to infinity!  Actually, as you can see from the plot below, it is effectively 0 much earlier than that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# from Andy Connolly\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def unitVolume(dimension, radius=1.):\n",
    "    return 2*(radius**dimension *np.pi**(dimension/2.))/(dimension*sp.gamma(dimension/2.))\n",
    "\n",
    "dim = np.linspace(1,100)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(dim,unitVolume(dim)/2.**dim)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('$Dimension$')\n",
    "ax.set_ylabel('$Volume$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that this works in the opposite direction too: let's say you want to find \"rare\" objects in 10 dimensions, where we'll define rare as <1% of the population.  Then you'll need to accept objects from 63% of the distribution in all 10 dimensions!  So are those really \"rare\" or are they just a particular 1% of the population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63095734448\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "p = 10**(np.log10(0.01)/10.0)\n",
    "print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "N.B.  Dimensionality isn't just measuring $D$ parameters for $N$ objects.  It could be a spectrum with $D$ values or an image with $D$ pixels, etc.  In the book the examples used just happen to be spectra of galaxies from the SDSS project.  But we can insert the data of our choice instead.\n",
    "\n",
    "For example: the SDSS comprises a sample of 357 million sources: \n",
    "- each source has 448 measured attributes\n",
    "- selecting just 30 (e.g., magnitude, size..) and normalizing the data range $-1$ to $1$\n",
    "\n",
    "yields a probability of having one of the 357 million sources reside within a unit hypersphere of 1 in 1.4$\\times 10^5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "In [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) we seek to take a data set like the one shown below and apply a transform to the data such that the new axes are aligned with the maximal variance of the data.  As can be seen in the Figure, this is basically just the same as doing regression by minimizing the square of the perpendicular distances to the new axes.  Note that we haven't made any changes to the data, we have just defined new axes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 7.2\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set parameters and draw the random sample\n",
    "np.random.seed(42)\n",
    "r = 0.9\n",
    "\n",
    "sigma1 = 0.25\n",
    "sigma2 = 0.08\n",
    "rotation = np.pi / 6\n",
    "s = np.sin(rotation)\n",
    "c = np.cos(rotation)\n",
    "\n",
    "X = np.random.normal(0, [sigma1, sigma2], size=(100, 2)).T\n",
    "R = np.array([[c, -s],[s, c]])\n",
    "X = np.dot(R, X)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the diagram\n",
    "fig = plt.figure(figsize=(5, 5), facecolor='w')\n",
    "ax = plt.axes((0, 0, 1, 1), xticks=[], yticks=[], frameon=False)\n",
    "\n",
    "# draw axes\n",
    "ax.annotate(r'$x$', (-r, 0), (r, 0),\n",
    "            ha='center', va='center',\n",
    "            arrowprops=dict(arrowstyle='<->', color='k', lw=1))\n",
    "ax.annotate(r'$y$', (0, -r), (0, r),\n",
    "            ha='center', va='center',\n",
    "            arrowprops=dict(arrowstyle='<->', color='k', lw=1))\n",
    "\n",
    "# draw rotated axes\n",
    "ax.annotate(r'$x^\\prime$', (-r * c, -r * s), (r * c, r * s),\n",
    "            ha='center', va='center',\n",
    "            arrowprops=dict(color='k', arrowstyle='<->', lw=1))\n",
    "ax.annotate(r'$y^\\prime$', (r * s, -r * c), (-r * s, r * c),\n",
    "            ha='center', va='center',\n",
    "            arrowprops=dict(color='k', arrowstyle='<->', lw=1))\n",
    "\n",
    "# scatter points\n",
    "ax.scatter(X[0], X[1], s=25, lw=0, c='k', zorder=2)\n",
    "\n",
    "# draw lines\n",
    "vnorm = np.array([s, -c])\n",
    "for v in (X.T):\n",
    "    d = np.dot(v, vnorm)\n",
    "    v1 = v - d * vnorm\n",
    "    ax.plot([v[0], v1[0]], [v[1], v1[1]], '-k')\n",
    "\n",
    "# draw ellipses\n",
    "for sigma in (1, 2, 3):\n",
    "    ax.add_patch(Ellipse((0, 0), 2 * sigma * sigma1, 2 * sigma * sigma2,\n",
    "                         rotation * 180. / np.pi,\n",
    "                         ec='k', fc='gray', alpha=0.2, zorder=1))\n",
    "\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the points are correlated along a particular direction which doesn't align with the initial choice of axes.  So, we should rotate our axes to align with this correlation. \n",
    "\n",
    "We'll choose the rotation to maximize the ability to discriminate between the data points:\n",
    "*   the first axis, or **principal component**, is direction of maximal variance\n",
    "*   the second principal component is orthogonal to the first component and maximizes the residual variance\n",
    "*   ...\n",
    "\n",
    "PCA is a dimensional reduction process because we can generally account for nearly \"all\" of the variance in the data set with fewer than the original $K$ dimensions.  See more below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We start with a data set $\\{x_i\\}$ which consists of $N$ objects for which we measure $K$ features.  We start by subtracting the mean for each feature in $\\{x_i\\}$ and write $X$ as a $N\\times K$ matrix.\n",
    "\n",
    "The covariance of this matrix is \n",
    "$$C_X=\\frac{1}{N-1}X^TX.$$\n",
    "\n",
    "There are off-diagonal terms if there are correlations between the measurements (e.g., maybe two of the features are temperature dependent and the measurements were taken at the same time).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $R$ is a projection of the data that is aligned with the maximal variance, then we have $Y= X R$ with covariance \n",
    "\n",
    "$$ C_{Y} = R^T X^T X R = R^T C_X R.$$\n",
    "\n",
    "$r_1$ is the first principal component of $R$, which can be derived using Langrange multipliers with the following cost function:\n",
    "\n",
    "$$ \\phi(r_1,\\lambda_1) = r_1^TC_X r_1 - \\lambda_1(r_1^Tr_1-1). $$\n",
    "\n",
    "If we take derivative of $\\phi(r_1,\\lambda)$ with respect to $r_1$ and set it to 0, then we have\n",
    "\n",
    "$$ C_Xr_1 - \\lambda_1 r_1 = 0. $$\n",
    "\n",
    "$\\lambda_1$ (the largest eigenvalue of the matrix) is the root of the equation $\\det(C_X -\n",
    "\\lambda_1 {\\bf I})=0$ for which the eigenvalue is\n",
    "\n",
    "$$ \\lambda_1 =  r_1^T C_X r_1.$$\n",
    "\n",
    "The columns of the full matrix, $R$ are the eigenvectors (known here as principal components)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We aren't going to go through the linear algebra more than that here.  But it would be a good group project for someone.  See the end of 7.3.1 starting at the bottom on page 294 or go through [Karen Leighly's PCA lecture notes](http://seminar.ouml.org/lectures/principal-components-analysis/) if you want to walk through the math in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preparing data for PCA\n",
    "\n",
    "   * Subtract the mean of each dimension (to \"center\" the data)\n",
    "   * Divide by the variance in each dimension (to \"whiten\" the data)\n",
    "   * (For spectra and images) normalize each row to yield an integral of unity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Example call from 7.3.2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = np.random.normal(size=(100,3)) # 100 points in 3D\n",
    "R = np.random.random((3,10)) # projection matrix\n",
    "X = np.dot(X,R) # X is now 10-dim, with 5 intrinsic dims\n",
    "pca = PCA(n_components=4) # n_components can be optionally set\n",
    "pca.fit(X) \n",
    "comp = pca.transform(X) # compute the subspace projection of X\n",
    "mean = pca.mean_ # length 10 mean of the data\n",
    "components = pca.components_ # 4x10 matrix of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Scikit-Learn's decomposition module](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) has a number of [PCA type implementations](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA).\n",
    "Let's work through an example using spectra of galaxies take during the Sloan Digital Sky Survey.  In this sample there are 4000 spectra with flux measurements in 1000 bins.  15 example spectra are shown below and our example will use half of the spectra chosen at random.  \n",
    "\n",
    "![Ivezic, Figure 7.1](http://www.astroml.org/_images/fig_spec_examples_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Example from Andy Connolly\n",
    "# See Ivezic, Figure 7.4\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "print len(spectra), len(wavelengths)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Compute PCA\n",
    "np.random.seed(500)\n",
    "nrows = 2000 # We'll just look at 2000 random spectra\n",
    "n_components = 5 # Do the fit with 5 components, which is the mean plus 4\n",
    "ind = np.random.randint(spectra.shape[0], size=nrows)\n",
    "\n",
    "spec_mean = spectra[ind].mean(0) # Compute the mean spectrum, which is the first component\n",
    "# spec_mean = spectra[:50].mean(0)\n",
    "\n",
    "# use Randomized PCA for speed\n",
    "pca = RandomizedPCA(n_components - 1)\n",
    "pca.fit(spectra[ind])\n",
    "pca_comp = np.vstack([spec_mean,pca.components_]) #Add the mean to the components\n",
    "evals = pca.explained_variance_ratio_ \n",
    "print evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's plot the components.  See also Ivezic, Figure 7.4.  The left hand panels are just the first 5 spectra for comparison with the first 5 PCA components, which are shown on the right.  They are ordered by the size of their eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Make plots\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "titles = 'PCA components'\n",
    "\n",
    "for j in range(n_components):\n",
    "    \n",
    "    # plot the components\n",
    "    ax = fig.add_subplot(n_components, 2, 2*j+2)\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax.set_xlabel('wavelength (Angstroms)')\n",
    "    ax.plot(wavelengths, pca_comp[j], '-k', lw=1)\n",
    "\n",
    "    # plot zero line\n",
    "    xlim = [3000, 7999]\n",
    "    ax.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "    \n",
    "    # plot the first j spectra\n",
    "    ax2 = fig.add_subplot(n_components, 2, 2*j+1)\n",
    "    ax2.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax2.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax2.set_xlabel('wavelength (Angstroms)')\n",
    "    ax2.plot(wavelengths, spectra[j], '-k', lw=1)\n",
    "    \n",
    "    # plot zero line\n",
    "    ax2.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax2.set_xlim(xlim)\n",
    "\n",
    "    if j == 0:\n",
    "        ax.set_title(titles, fontsize='medium')\n",
    "\n",
    "    if j == 0:\n",
    "        label = 'mean'\n",
    "    else:\n",
    "        label = 'component %i' % j\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax2.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax.text(0.02, 0.95, label, transform=ax.transAxes,\n",
    "            ha='left', va='top', bbox=dict(ec='w', fc='w'),\n",
    "            fontsize='small')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's make \"scree\" plots.  These plots tell us how much of the variance is explained as a function of the each eigenvector.  Our plot won't look much like Ivezic, Figure 7.5, so I've shown it below to explain where \"scree\" comes from.\n",
    "![Ivezic, Figure 7.5](http://www.astroml.org/_images/fig_eigenvalues_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(np.arange(n_components-1), evals)\n",
    "ax.set_xlabel(\"eigenvalue number\")\n",
    "ax.set_ylabel(\"eigenvalue \")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(np.arange(n_components-1), evals.cumsum())\n",
    "ax.set_xlabel(\"eigenvalue number\")\n",
    "ax.set_ylabel(\"cumulative eigenvalue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How much of the variance is explained by the first two components?  How about all of the components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The first component explains {:.3f} of the variance in the data.\".format(# complete\n",
    "print(\"The second component explains {:.3f} of the variance in the data.\".format(# complete\n",
    "print(\"All components explain {:.3f} of the variance in the data.\".format(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is why PCA enables dimensionality reduction.\n",
    "\n",
    "How many components would we need to explain 99.5% of the variance?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for num_feats in np.arange(1,20, dtype = int):\n",
    "    # complete\n",
    "print(\"{:d} features are needed to explain 99.5% of the variance\".format(# Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that we might need as many as 1000 to encode all of the variance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting the PCA\n",
    "\n",
    "- The output eigenvectors are ordered by their associated eigenvalues \n",
    "- The eigenvalues reflect the variance within each eigenvector\n",
    "- The sum of the eigenvalues is total variance of the system\n",
    "- Projection of each spectrum onto the first few eigenspectra is a compression of the data \n",
    "\n",
    "Once we have the eigenvectors, we can try to reconstruct an observed spectrum, ${x}(k)$, in the eigenvector basis, ${e}_i(k)$, as \n",
    "\n",
    "$$ \\begin{equation}\n",
    "  {x}_i(k) = {\\mu}(k) + \\sum_j^R \\theta_{ij} {e}_j(k).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "That would give a full (perfect) reconstruction of the data since it uses all of the eigenvectors.  But if we truncate (i.e., $r<R$), then we will have reduced the dimensionality while still reconstructing the data with relatively little loss of information.\n",
    "\n",
    "For example, we started with 4000x1000 floating point numbers.  If we can explain nearly all of the variance with 8 eigenvectors, then we have reduced the problem to 4000x8+8x1000 floating point numbers!\n",
    "\n",
    "Execute the next cell to see how the reconstruction improves by adding more components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute PCA components\n",
    "\n",
    "# Eigenvalues can be computed using PCA as in the commented code below:\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA()\n",
    "#pca.fit(spectra)\n",
    "#evals = pca.explained_variance_ratio_\n",
    "#evals_cs = evals.cumsum()\n",
    "\n",
    "#  because the spectra have been reconstructed from masked values, this\n",
    "#  is not exactly correct in this case: we'll use the values computed\n",
    "#  in the file compute_sdss_pca.py\n",
    "evals = data['evals'] ** 2\n",
    "evals_cs = evals.cumsum()\n",
    "evals_cs /= evals_cs[-1]\n",
    "evecs = data['evecs']\n",
    "spec_mean = spectra.mean(0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Find the coefficients of a particular spectrum\n",
    "spec = spectra[1]\n",
    "coeff = np.dot(evecs, spec - spec_mean)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the sequence of reconstructions\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "\n",
    "for i, n in enumerate([0, 4, 8, 20]):\n",
    "    ax = fig.add_subplot(411 + i)\n",
    "    ax.plot(wavelengths, spec, '-', c='gray')\n",
    "    ax.plot(wavelengths, spec_mean + np.dot(coeff[:n], evecs[:n]), '-k')\n",
    "\n",
    "    if i < 3:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    ax.set_ylim(-2, 21)\n",
    "    ax.set_ylabel('flux')\n",
    "\n",
    "    if n == 0:\n",
    "        text = \"mean\"\n",
    "    elif n == 1:\n",
    "        text = \"mean + 1 component\\n\"\n",
    "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "    else:\n",
    "        text = \"mean + %i components\\n\" % n\n",
    "        text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "\n",
    "    ax.text(0.01, 0.95, text, ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "fig.axes[-1].set_xlabel(r'${\\rm wavelength\\ (\\AA)}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caveats I\n",
    "\n",
    "PCA is a linear process, whereas the variations in the data may not be.  So it may not always be appropriate to use and/or may require a relatively large number of components to fully describe any non-linearity.\n",
    "\n",
    "Note also that PCA can be very impractical for large data sets which exceed the memory per core as the computational requirement goes as $\\mathscr{O}(D^3$) and the memory requirement goes as $\\mathscr{O}(2D^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Missing Data\n",
    "We have assumed so far that there is no missing data (e.g., bad pixels in the spectrum, etc.).  But often the data set is incomplete.  Since PCA encodes the flux correlation with wavelength (or whatever parameters are in your data set), we can actually use it to determine missing values.  \n",
    "\n",
    "An example is shown below.  Here, black are the observed spectra.  Gray are the regions where we have no data.  Blue is the PCA reconstruction, including the regions where there are no data.  Awesome, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "from astroML.datasets import fetch_sdss_corrected_spectra\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get spectra and eigenvectors used to reconstruct them\n",
    "data = fetch_sdss_corrected_spectra()\n",
    "spec = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "lam = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "evecs = data['evecs']\n",
    "mu = data['mu']\n",
    "norms = data['norms']\n",
    "mask = data['mask']\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "i_plot = ((lam > 5750) & (lam < 6350))\n",
    "lam = lam[i_plot]\n",
    "\n",
    "specnums = [20, 8, 9]\n",
    "subplots = [311, 312, 313]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 10))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "\n",
    "for subplot, i in zip(subplots, specnums):\n",
    "    ax = fig.add_subplot(subplot)\n",
    "\n",
    "    # compute eigen-coefficients\n",
    "    spec_i_centered = spec[i] / norms[i] - mu\n",
    "    coeffs = np.dot(spec_i_centered, evecs.T)\n",
    "\n",
    "    # blank out masked regions\n",
    "    spec_i = spec[i]\n",
    "    mask_i = mask[i]\n",
    "    spec_i[mask_i] = np.nan\n",
    "\n",
    "    # plot the raw masked spectrum\n",
    "    ax.plot(lam, spec_i[i_plot], '-', color='k', lw=2,\n",
    "            label='True spectrum')\n",
    "\n",
    "    # plot two levels of reconstruction\n",
    "    for nev in [10]:\n",
    "        if nev == 0:\n",
    "            label = 'mean'\n",
    "        else:\n",
    "            label = 'N EV=%i' % nev\n",
    "        spec_i_recons = norms[i] * (mu + np.dot(coeffs[:nev], evecs[:nev]))\n",
    "        ax.plot(lam, spec_i_recons[i_plot], label=label)\n",
    "\n",
    "    # plot shaded background in masked region\n",
    "    ylim = ax.get_ylim()\n",
    "    mask_shade = ylim[0] + mask[i][i_plot].astype(float) * ylim[1]\n",
    "    plt.fill(np.concatenate([lam[:1], lam, lam[-1:]]),\n",
    "             np.concatenate([[ylim[0]], mask_shade, [ylim[0]]]),\n",
    "             lw=0, fc='k', alpha=0.2)\n",
    "\n",
    "    ax.set_xlim(lam[0], lam[-1])\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "\n",
    "    if subplot == 311:\n",
    "        ax.legend(loc=1, prop=dict(size=14))\n",
    "\n",
    "    ax.set_xlabel('$\\lambda\\ (\\AA)$')\n",
    "    ax.set_ylabel('normalized flux')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The example that we have been using above is \"spectral\" PCA.  Some examples from the literature include:\n",
    "- [Francis et al. 1992](http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1992ApJ...398..476F&amp;data_type=PDF_HIGH&amp;whole_paper=YES&amp;type=PRINTER&amp;filetype=.pdf)\n",
    "- [Connolly et al. 1995](http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1995AJ....110.1071C&amp;data_type=PDF_HIGH&amp;whole_paper=YES&amp;type=PRINTER&amp;filetype=.pdf)\n",
    "- [Yip et al. 2004](http://iopscience.iop.org/article/10.1086/425626/meta;jsessionid=31BB5F11B85D2BF4180834DC71BA0B85.c3.iopscience.cld.iop.org)\n",
    "\n",
    "One can also do PCA on features that aren't ordered (as they were for the spectra).  E.g., if you have $D$ different parameters measured for your objects.  The classic example in astronomy is\n",
    "[Boroson & Green 1992](http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1992ApJS...80..109B&amp;data_type=PDF_HIGH&amp;whole_paper=YES&amp;type=PRINTER&amp;filetype=.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caveats II\n",
    "\n",
    "One of the things that I don't like about PCA is that the eigenvectors are just mathematical constructs.  They often don't look anything like the spectra themselves.  Whereas it is often the case that you might expect that the components would look like, well, the physical components.  For example, quasars are fundamentally galaxies.  So, part of their flux comes from the galaxy that they live in.  But PCA doesn't return any component that looks like a typical galaxy.  Essentially this is because the components can be both positive and negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "This is where [Non-negative Matrix Factorizaiton (NMF)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) comes in.  Here we are treating the data as a linear sum of positive-definite components. \n",
    "\n",
    "NMF assumes  any data matrix can be factored into two matrices, $W$ and $Y$, with\n",
    "$$\\begin{equation}\n",
    "X=W Y,\n",
    "\\end{equation}\n",
    "$$\n",
    "where both $W$ and $Y$ are nonnegative. \n",
    "\n",
    "So, $WY$ is an approximation of $X$. Minimizing the reconstruction error $|| (X - W Y)^2 ||$, \n",
    "nonnegative bases can be derived through an iterative process.\n",
    "\n",
    "Note, however, that the iterative process does not guarantee nonlocal minima (like $K$-means and EM), but using \n",
    "random initialization and cross-validation can be used to find the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example from the literature is [Allen et al. 2008](http://arxiv.org/abs/0810.4231)\n",
    "\n",
    "In Scikit-Learn the [NMF implementation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "X = np.random.random((100,3)) # 100 points in 3D\n",
    "nmf = NMF(n_components=3)\n",
    "nmf.fit(X)\n",
    "proj = nmf.transform(X) # project to 3 dimension\n",
    "comp = nmf.components_ # 3x10 array of components\n",
    "err = nmf.reconstruction_err_ # how well 3 components capture the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example (and comparison to PCA) is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the next 2 cells\n",
    "# Example from Figure 7.4\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Compute PCA, and NMF components\n",
    "def compute_PCA_NMF(n_components=5):\n",
    "    spec_mean = spectra.mean(0)\n",
    "\n",
    "    # PCA: use randomized PCA for speed\n",
    "    pca = RandomizedPCA(n_components - 1)\n",
    "    pca.fit(spectra)\n",
    "    pca_comp = np.vstack([spec_mean,\n",
    "                          pca.components_])\n",
    "\n",
    "\n",
    "    # NMF requires all elements of the input to be greater than zero\n",
    "    spectra[spectra < 0] = 0\n",
    "    nmf = NMF(n_components)\n",
    "    nmf.fit(spectra)\n",
    "    nmf_comp = nmf.components_\n",
    "\n",
    "    return pca_comp, nmf_comp\n",
    "\n",
    "n_components = 5\n",
    "decompositions = compute_PCA_NMF(n_components)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "titles = ['PCA components', 'NMF components']\n",
    "\n",
    "for i, comp in enumerate(decompositions):\n",
    "    for j in range(n_components):\n",
    "        ax = fig.add_subplot(n_components, 3, 3 * j + 1 + i)\n",
    "\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "        if j < n_components - 1:\n",
    "            ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        else:\n",
    "            ax.set_xlabel('wavelength (Angstroms)')\n",
    "\n",
    "        ax.plot(wavelengths, comp[j], '-k', lw=1)\n",
    "\n",
    "        # plot zero line\n",
    "        xlim = [3000, 7999]\n",
    "        ax.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "        if j == 0:\n",
    "            ax.set_title(titles[i])\n",
    "\n",
    "        if titles[i].startswith('PCA') or titles[i].startswith('ICA'):\n",
    "            if j == 0:\n",
    "                label = 'mean'\n",
    "            else:\n",
    "                label = 'component %i' % j\n",
    "        else:\n",
    "            label = 'component %i' % (j + 1)\n",
    "\n",
    "        ax.text(0.03, 0.94, label, transform=ax.transAxes,\n",
    "                ha='left', va='top')\n",
    "\n",
    "        for l in ax.get_xticklines() + ax.get_yticklines(): \n",
    "            l.set_markersize(2) \n",
    "\n",
    "        # adjust y limits\n",
    "        ylim = plt.ylim()\n",
    "        dy = 0.05 * (ylim[1] - ylim[0])\n",
    "        ax.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independent Component Analysis (ICA)\n",
    "\n",
    "For data where the components are statistically independent (or nearly so) [Independent Component Analysis (ICA)](https://en.wikipedia.org/wiki/Independent_component_analysis) has become a popular method for separating mixed components.  The classical example is the so-called \"cocktail party\" problem.  This is illustrated in the following figure from Hastie, Tibshirani, and Friedman (Figure 14.27 on page 497 in my copy, so they have clearly added some stuff!).  Think of the \"source signals\" as two voices at a party.  You are trying to concentrate on just one voice.  What you hear is something like the \"measured signals\" pattern.  You could run the data through PCA and that would do an excellent job of reconstructing the signal with reduced dimensionality, but it wouldn't actually isolate the different physical components (bottom-left panel)  ICA on the other hand can (bottom-right panel).\n",
    "\n",
    "![Cocktail Party Illustration from Hastie's book, Figure 14.37.  You can see this on page 580 of [Hastie, Tibshirani, and Friedman pdf](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf).](images/HastieFigure14_37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ICA is a good choice for a complex system with relatively indepent components.  For example a galaxy is roughly a linear combination of cool stars and hot stars, and a quasar is just a galaxy with others component from an accretion disk and emission line regions.  Ideally we want \"eigenvectors\" that are aligned with those physical traits/regions as opposed to mathematical constructs.\n",
    "\n",
    "The basic call to the [FastICA algoirthm](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html) in Scikit-Learn looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA\n",
    "X = np.random.normal(size=(100,2)) # 100 objects in 2D\n",
    "R = np.random.random((2,5)) # mixing matrix\n",
    "X = np.dot(X,R) # 2D data in 5D space\n",
    "ica = FastICA(2) # fit 2 components\n",
    "ica.fit(X)\n",
    "proj = ica.transform(X) # 100x2 projection of the data\n",
    "comp = ica.components_ # 2x5 matrix of independent components\n",
    "## sources = ica.sources_ # 100x2 matrix of sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Execute the next 2 cells to produce a plot showing the ICA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#Example from Andy Connolly\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Download data\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "spectra = sdss_corrected_spectra.reconstruct_spectra(data)\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Compute PCA\n",
    "np.random.seed(500)\n",
    "nrows = 500\n",
    "n_components = 5\n",
    "ind = np.random.randint(spectra.shape[0], size=nrows)\n",
    "\n",
    "\n",
    "spec_mean = spectra[ind].mean(0)\n",
    "# spec_mean = spectra[:50].mean(0)\n",
    "\n",
    "ica = FastICA(n_components - 1)\n",
    "ica.fit(spectra[ind])\n",
    "ica_comp = np.vstack([spec_mean,ica.components_]) #Add the mean to the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Make plots\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,\n",
    "                    bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "titles = 'ICA components'\n",
    "\n",
    "for j in range(n_components):\n",
    "    \n",
    "    # plot the components\n",
    "    ax = fig.add_subplot(n_components, 2, 2*j+2)\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax.plot(wavelengths, ica_comp[j], '-k', lw=1)\n",
    "\n",
    "    # plot zero line\n",
    "    xlim = [3000, 7999]\n",
    "    ax.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "    # plot the first j spectra\n",
    "    ax2 = fig.add_subplot(n_components, 2, 2*j+1)\n",
    "    ax2.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax2.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "    if j < n_components - 1:\n",
    "        ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax2.set_xlabel(r'wavelength ${\\rm (\\AA)}$')\n",
    "    ax2.plot(wavelengths, spectra[j], '-k', lw=1)\n",
    "    \n",
    "    # plot zero line\n",
    "    ax2.plot(xlim, [0, 0], '-', c='gray', lw=1)\n",
    "    ax2.set_xlim(xlim)\n",
    "\n",
    "    if j == 0:\n",
    "        ax.set_title(titles, fontsize='medium')\n",
    "\n",
    "    if j == 0:\n",
    "        label = 'mean'\n",
    "    else:\n",
    "        label = 'component %i' % j\n",
    "\n",
    "    # adjust y limits\n",
    "    ylim = plt.ylim()\n",
    "    dy = 0.05 * (ylim[1] - ylim[0])    \n",
    "    ax2.set_ylim(ylim[0] - dy, ylim[1] + 4 * dy)\n",
    "\n",
    "\n",
    "    ax.text(0.02, 0.95, label, transform=ax.transAxes,\n",
    "            ha='left', va='top', bbox=dict(ec='w', fc='w'),\n",
    "            fontsize='small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As with PCA and NMF, we can similarly do a reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "#------------------------------------------------------------\n",
    "# Find the coefficients of a particular spectrum\n",
    "spec = spectra[1]\n",
    "evecs = data['evecs']\n",
    "coeff = np.dot(evecs, spec - spec_mean)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the sequence of reconstructions\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "\n",
    "for i, n in enumerate([0, 2, 4, 8]):\n",
    "    ax = fig.add_subplot(411 + i)\n",
    "    ax.plot(wavelengths, spec, '-', c='gray')\n",
    "    ax.plot(wavelengths, spec_mean + np.dot(coeff[:n], evecs[:n]), '-k')\n",
    "\n",
    "    if i < 3:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    ax.set_ylim(-2, 21)\n",
    "    ax.set_ylabel('flux')\n",
    "\n",
    "    if n == 0:\n",
    "        text = \"mean\"\n",
    "    elif n == 1:\n",
    "        text = \"mean + 1 component\\n\"\n",
    "        #text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "    else:\n",
    "        text = \"mean + %i components\\n\" % n\n",
    "        #text += r\"$(\\sigma^2_{tot} = %.2f)$\" % evals_cs[n - 1]\n",
    "\n",
    "    ## GTR: had to comment this out for some reason\n",
    "    ## ax.text(0.01, 0.95, text, ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "fig.axes[-1].set_xlabel(r'${\\rm wavelength\\ (\\AA)}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ivezic, Figure 7.4 compares the components found by the PCA, ICA, and NMF algorithms.  Their differences and similarities are quite interesting.\n",
    "\n",
    "![Ivezic, Figure 7.4](http://www.astroml.org/_images/fig_spec_decompositions_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you think that I was pulling your leg about the cocktail problem, try it yourself!\n",
    "Load the code instead of running it and see what effect changing some things has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%run code/plot_ica_blind_source_separation.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
